{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"WRO Future Engineers Getting Started Starting from the season 2020 World Robot Olympiad extends the set of the competitions suggested for school students around the world by a new competition \"Future Engineers\". The WRO Future Engineers is the competition of the self-driven vehicles. Students of the age group from 15 till 19 need to design a model of a car, equip it with electromechanical components and program it as so it will be able to autonomously drive on the track avoiding objections. More details about the competition can be found on the official site of WRO Association . The Future Engineers challenge does not limit the students with any specific hardware and programming languages that is why working with electromechanical components (motors and sensors) and/or with a micro controller could cause some difficulties. Moreover, the most effective way to make the car fully autonomous is to use a camera and computer vision algorithms which are also completely new direction in WRO competition and never used before. Therefore the goal of this site is to provide materials for an initial understanding how the self-driving vehicle can be assembled and how it can be programmed to achieve a result in the competition. This site covers the following topics: Chassis -- how to assemble a vehicle Electromechanical components -- which electromechanical components and controllers can be used and how to connect them to each other Basics of SBM and SBC programming -- materials describing different aspects of programming Arduino and Raspberry Pi boards Computer vision -- a pipeline that can be used to implement an algorithm to solve the challenge Miscellaneous -- extra materials that do not fit in other sections Robot sets -- robot sets that can be used for a quick start Enhoy with getting new knowledge and skills! This site was generated from the content located in the GitHub repository: https://github.com/World-Robot-Olympiad-Association/future-engineers-gs . If you have any suggestions or found some bugs/inconsistencies please report them in form of the GitHub issue .","title":"WRO Future Engineers Getting Started"},{"location":"#wro-future-engineers-getting-started","text":"Starting from the season 2020 World Robot Olympiad extends the set of the competitions suggested for school students around the world by a new competition \"Future Engineers\". The WRO Future Engineers is the competition of the self-driven vehicles. Students of the age group from 15 till 19 need to design a model of a car, equip it with electromechanical components and program it as so it will be able to autonomously drive on the track avoiding objections. More details about the competition can be found on the official site of WRO Association . The Future Engineers challenge does not limit the students with any specific hardware and programming languages that is why working with electromechanical components (motors and sensors) and/or with a micro controller could cause some difficulties. Moreover, the most effective way to make the car fully autonomous is to use a camera and computer vision algorithms which are also completely new direction in WRO competition and never used before. Therefore the goal of this site is to provide materials for an initial understanding how the self-driving vehicle can be assembled and how it can be programmed to achieve a result in the competition. This site covers the following topics: Chassis -- how to assemble a vehicle Electromechanical components -- which electromechanical components and controllers can be used and how to connect them to each other Basics of SBM and SBC programming -- materials describing different aspects of programming Arduino and Raspberry Pi boards Computer vision -- a pipeline that can be used to implement an algorithm to solve the challenge Miscellaneous -- extra materials that do not fit in other sections Robot sets -- robot sets that can be used for a quick start Enhoy with getting new knowledge and skills! This site was generated from the content located in the GitHub repository: https://github.com/World-Robot-Olympiad-Association/future-engineers-gs . If you have any suggestions or found some bugs/inconsistencies please report them in form of the GitHub issue .","title":"WRO Future Engineers Getting Started"},{"location":"p01-chassis/","text":"Chassis This section covers such aspects of building the vehicle for the competition as a chassis and tires. Steering mechanism Even if the game rules of the Future Engineering competition does not require to build your own vehicle and allow to use a base of an RC (remote controlled) toy cars, someone would find useful to design everything from very beginning. It will allow to get grain control over the hardware, increase efficiency and reduce errors introduced by the third party's implementations. The design of the vehicle is dictated by the approach is chosen to drive the rear wheels and implement the steering mechanics. Usually, in the most simple case the rear wheels are fixed on the same axle and rotated by one DC motor. The steering mechanism on the front axle could be build in several ways where the main idea is to change of the angle of the front wheels direction. Since these concepts are already widely implemented in the RC toy cars and the RC sport vehicles, it makes sense to make a research through available materials. For example, this video presents one of the way to build the steering mechanism: More videos referenced in the next list consider other approaches to acheive the same goal: 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 . When you get a basic understanding how your vehicle could look like, you can look at the following instructions that could help to think of the assembling process in more professional manner: Click on the picture or here to download the instructions. Note, that some instructions above uses two motors to rotate the rear wheels. In some cases it will make the control of the vehicle more complex since the motors could rotate asynchronously that needs to be compensated by the steering. Tires By the way, during experiments you can find that the tires in the vehicle have no enough friction with the field surface, so it make sense to consider to try to prepare your own tires for wheels. The following links could bring other brilliant ideas how to make the tires for the vehicle: 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 .","title":"Chassis"},{"location":"p01-chassis/#chassis","text":"This section covers such aspects of building the vehicle for the competition as a chassis and tires.","title":"Chassis"},{"location":"p01-chassis/#steering-mechanism","text":"Even if the game rules of the Future Engineering competition does not require to build your own vehicle and allow to use a base of an RC (remote controlled) toy cars, someone would find useful to design everything from very beginning. It will allow to get grain control over the hardware, increase efficiency and reduce errors introduced by the third party's implementations. The design of the vehicle is dictated by the approach is chosen to drive the rear wheels and implement the steering mechanics. Usually, in the most simple case the rear wheels are fixed on the same axle and rotated by one DC motor. The steering mechanism on the front axle could be build in several ways where the main idea is to change of the angle of the front wheels direction. Since these concepts are already widely implemented in the RC toy cars and the RC sport vehicles, it makes sense to make a research through available materials. For example, this video presents one of the way to build the steering mechanism: More videos referenced in the next list consider other approaches to acheive the same goal: 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 . When you get a basic understanding how your vehicle could look like, you can look at the following instructions that could help to think of the assembling process in more professional manner: Click on the picture or here to download the instructions. Note, that some instructions above uses two motors to rotate the rear wheels. In some cases it will make the control of the vehicle more complex since the motors could rotate asynchronously that needs to be compensated by the steering.","title":"Steering mechanism"},{"location":"p01-chassis/#tires","text":"By the way, during experiments you can find that the tires in the vehicle have no enough friction with the field surface, so it make sense to consider to try to prepare your own tires for wheels. The following links could bring other brilliant ideas how to make the tires for the vehicle: 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 .","title":"Tires"},{"location":"p02-electronics/","text":"Electromechanical components Here are materials that aimed to help understand how to make the vehicle is smarter than an RC toy. Design leading questions Definitely, the obstacles detection and decision making for the motion planning must be done ad-hoc by en electronic brain rather than by a human being. That's why it is needed to choose the set of controllers, electronic components and design the logic to connect them to each other. They will mostly rely on the way how the vehicle will solve the task. But some ideas will be common due to the nature of the challenge. Here is the set of questions that is intended to boost the process of thinking about electronic filling of the autonomous car: Will a camera be used to recognize the traffic signs (red and green pillars)? Is it an USB-camera or a natively plugged device (like the Raspberry Pi camera)? How fast the video stream from the camera must be handled? What is suitable performance of the decision making controller? Will the vehicle be controlled by one controller or by a pair of the main controller and an auxiliary controller? How will the roles between controllers be distributed: handling of the video stream, direct control of the motors, direct acquiring of sensors readings? Will capacity (input/output pins) of each controller be enough for this? What kind of connection will be between the controllers? Will the walls be detected by camera or by a distance/proximity sensors? Should a special lens kit be used to make the camera's field of view wider to recognize both traffic signs and walls or several cameras will be used instead? How will the passed path be measured: by counting numbers of turns with a compass or a gyroscope, by calculating the distance with an encoder(s), by detecting colored lines with light sensors? Will a gyroscope and/or an accelerometer be used to assist in accurate driving -- get rid of excessive steering mechanism shifts? What kind of power sources should be used to supply controllers, motors and senors? Should it be one source for all consumers or dedicated sources for controllers and motors? Overview Before start drawing schemes and purchasing the components, usually it is worth to review existing solutions: what electromechanical components are the common practice to use for the robotics projects where the aim is to create a self-driving vehicle. This is a short collection of the links to study. Descriptions of these projects provides different level of details that's why we list several of them to demonstrate diversity of approaches: Is your toy RC car is good to become autonomous robotics car? A good start to turn an RC car to Arduino controlled car An example of equipping an RC car with sensors and Arduino controller Not sure how to start driving autonomously? Start with these fours simple turtle-like car based on Arduino: 1 , 2 , 3 , 4 Yet another DIY autonomous vehicles but pay attention on the powering Vehicle's brain If this is the first time when you meet with Single Board Microcontrollers like Arduino or Single Board Computers like Raspberry Pi, consider to spend extra time to understand the difference between them. For example, the goal of this article to summarize advantages of SBM and SBC with advices when use Arduino, when Raspberry Pi and when both. Okay, hope the advantages are clear, and that is why it make sense to compare the Raspberry Pi with other SBCs: Click on the picture to watch the video Another interesting approach could be is to use and integrated Computer Vision boards instead of SBC. This material compares OpenMV, Pixy and Jevois. Someone already decides to build the vehicle with two controllers: the primary controller is to execute navigation, path planning and work with a camera, the secondary is to requests sensors and/or control motors. Most probably the secondary controller will be an Arduino based and the following step is learn how it could be connected to the primary one. There are several protocols that can be used for this and this document is good to start getting familiar with them. Next, depending on the protocol the following materials will help you implement communication channel between Raspberry Pi an Arduino: Serial communication: 1 , 2 , 3 , 4 . I2C communication: 1 , 2 , 3 . Motors The minimal number of motors that required for a vehicle participating in the Future Engineers competition is two: a driving motor and a motor to steer. Motor Basics Motor Driver Using a Servo Motor for the Steering The driving motor makes the car moving. This motors consumes more power than can be provided either by Arduino or by Raspberry Pi therefore an additional motor driver circuit is being used. Here are examples how to control DC motors from Arudino: 1 , 2 , 3 ; or from Raspberry Pi: 1 . In order to understand better how to control speed of a DC motor it makes sense to look at this article . As an alternative to the DC motors, one could consider to use a stepper motor or a continuous servomotor . A servo motor is the most suitable way to change direction of the vehicle since it provides precise control on the steering process. Here are examples how to control DC motors from Arudino: 1 , 2 , 3 , 4 ; or from Raspberry Pi: 1 . Encoders It is important to control the driving motor speed and the distance the vehicle is moving. One of the simplest device for this is an encoder: Click on the picture to watch the video These three articles will help you better understand how encoders work: 1 , 2 , 3 . And here are practical examples how an encoder can be added to the autonomous vehicle: Setup an encoder on a chassis Use encoder built in the motor Distance sensors Distance and proximity sensors are commonly used in robotics. For the autonomous vehicle participating in the Future Engineers competition such kind of devices will allow to control the distance to the walls as so the car will move in the middle between them. Here is a set of short reviews of different sensors (ultrasonic, infrared and LiDAR) describing the principles how they work and which advantages and disadvantages they have: 1 , 2 , 3 . IMU IMU is an abbreviation for the term Inertial Measurement Unit. Usually, this abbreviation is used for a device that combines accelerometer, gyroscope and magnetometer. In order to get a useful data from this sensor, in most cases it is required to implement concepts from the linear algebra and calculus. But depending on the libraries provided by the sensor manufactures and/or open source community, this could be already simplified a lot as so the IMU sensor can be used to measure the path driven by the vehicle and to control steering smoothness. We provide links to three articles that are good start to understand how IMU can be used in a robotics projects: 1 , 2 , 3 . Power Powering of the self-driving car for the robotics competition is not a trivial task. Depending on types of the motors and number of controllers, it could be found that separate power sources must be provided for different consumers. The materials below should allow you better understand approaches that can be used to power the controllers and motors: How to power the Arduino board: 1 , 2 How to power Raspberry Pi Powering Motors with Arduino Important note about the ground sharing Usage a power bank with Arduino and Motors Additional info to power the project","title":"Electronics"},{"location":"p02-electronics/#electromechanical-components","text":"Here are materials that aimed to help understand how to make the vehicle is smarter than an RC toy.","title":"Electromechanical components"},{"location":"p02-electronics/#design-leading-questions","text":"Definitely, the obstacles detection and decision making for the motion planning must be done ad-hoc by en electronic brain rather than by a human being. That's why it is needed to choose the set of controllers, electronic components and design the logic to connect them to each other. They will mostly rely on the way how the vehicle will solve the task. But some ideas will be common due to the nature of the challenge. Here is the set of questions that is intended to boost the process of thinking about electronic filling of the autonomous car: Will a camera be used to recognize the traffic signs (red and green pillars)? Is it an USB-camera or a natively plugged device (like the Raspberry Pi camera)? How fast the video stream from the camera must be handled? What is suitable performance of the decision making controller? Will the vehicle be controlled by one controller or by a pair of the main controller and an auxiliary controller? How will the roles between controllers be distributed: handling of the video stream, direct control of the motors, direct acquiring of sensors readings? Will capacity (input/output pins) of each controller be enough for this? What kind of connection will be between the controllers? Will the walls be detected by camera or by a distance/proximity sensors? Should a special lens kit be used to make the camera's field of view wider to recognize both traffic signs and walls or several cameras will be used instead? How will the passed path be measured: by counting numbers of turns with a compass or a gyroscope, by calculating the distance with an encoder(s), by detecting colored lines with light sensors? Will a gyroscope and/or an accelerometer be used to assist in accurate driving -- get rid of excessive steering mechanism shifts? What kind of power sources should be used to supply controllers, motors and senors? Should it be one source for all consumers or dedicated sources for controllers and motors?","title":"Design leading questions"},{"location":"p02-electronics/#overview","text":"Before start drawing schemes and purchasing the components, usually it is worth to review existing solutions: what electromechanical components are the common practice to use for the robotics projects where the aim is to create a self-driving vehicle. This is a short collection of the links to study. Descriptions of these projects provides different level of details that's why we list several of them to demonstrate diversity of approaches: Is your toy RC car is good to become autonomous robotics car? A good start to turn an RC car to Arduino controlled car An example of equipping an RC car with sensors and Arduino controller Not sure how to start driving autonomously? Start with these fours simple turtle-like car based on Arduino: 1 , 2 , 3 , 4 Yet another DIY autonomous vehicles but pay attention on the powering","title":"Overview"},{"location":"p02-electronics/#vehicles-brain","text":"If this is the first time when you meet with Single Board Microcontrollers like Arduino or Single Board Computers like Raspberry Pi, consider to spend extra time to understand the difference between them. For example, the goal of this article to summarize advantages of SBM and SBC with advices when use Arduino, when Raspberry Pi and when both. Okay, hope the advantages are clear, and that is why it make sense to compare the Raspberry Pi with other SBCs: Click on the picture to watch the video Another interesting approach could be is to use and integrated Computer Vision boards instead of SBC. This material compares OpenMV, Pixy and Jevois. Someone already decides to build the vehicle with two controllers: the primary controller is to execute navigation, path planning and work with a camera, the secondary is to requests sensors and/or control motors. Most probably the secondary controller will be an Arduino based and the following step is learn how it could be connected to the primary one. There are several protocols that can be used for this and this document is good to start getting familiar with them. Next, depending on the protocol the following materials will help you implement communication channel between Raspberry Pi an Arduino: Serial communication: 1 , 2 , 3 , 4 . I2C communication: 1 , 2 , 3 .","title":"Vehicle's brain"},{"location":"p02-electronics/#motors","text":"The minimal number of motors that required for a vehicle participating in the Future Engineers competition is two: a driving motor and a motor to steer. Motor Basics Motor Driver Using a Servo Motor for the Steering The driving motor makes the car moving. This motors consumes more power than can be provided either by Arduino or by Raspberry Pi therefore an additional motor driver circuit is being used. Here are examples how to control DC motors from Arudino: 1 , 2 , 3 ; or from Raspberry Pi: 1 . In order to understand better how to control speed of a DC motor it makes sense to look at this article . As an alternative to the DC motors, one could consider to use a stepper motor or a continuous servomotor . A servo motor is the most suitable way to change direction of the vehicle since it provides precise control on the steering process. Here are examples how to control DC motors from Arudino: 1 , 2 , 3 , 4 ; or from Raspberry Pi: 1 .","title":"Motors"},{"location":"p02-electronics/#encoders","text":"It is important to control the driving motor speed and the distance the vehicle is moving. One of the simplest device for this is an encoder: Click on the picture to watch the video These three articles will help you better understand how encoders work: 1 , 2 , 3 . And here are practical examples how an encoder can be added to the autonomous vehicle: Setup an encoder on a chassis Use encoder built in the motor","title":"Encoders"},{"location":"p02-electronics/#distance-sensors","text":"Distance and proximity sensors are commonly used in robotics. For the autonomous vehicle participating in the Future Engineers competition such kind of devices will allow to control the distance to the walls as so the car will move in the middle between them. Here is a set of short reviews of different sensors (ultrasonic, infrared and LiDAR) describing the principles how they work and which advantages and disadvantages they have: 1 , 2 , 3 .","title":"Distance sensors"},{"location":"p02-electronics/#imu","text":"IMU is an abbreviation for the term Inertial Measurement Unit. Usually, this abbreviation is used for a device that combines accelerometer, gyroscope and magnetometer. In order to get a useful data from this sensor, in most cases it is required to implement concepts from the linear algebra and calculus. But depending on the libraries provided by the sensor manufactures and/or open source community, this could be already simplified a lot as so the IMU sensor can be used to measure the path driven by the vehicle and to control steering smoothness. We provide links to three articles that are good start to understand how IMU can be used in a robotics projects: 1 , 2 , 3 .","title":"IMU"},{"location":"p02-electronics/#power","text":"Powering of the self-driving car for the robotics competition is not a trivial task. Depending on types of the motors and number of controllers, it could be found that separate power sources must be provided for different consumers. The materials below should allow you better understand approaches that can be used to power the controllers and motors: How to power the Arduino board: 1 , 2 How to power Raspberry Pi Powering Motors with Arduino Important note about the ground sharing Usage a power bank with Arduino and Motors Additional info to power the project","title":"Power"},{"location":"p03-programming/","text":"How to program This section provides links on the materials describing different aspects of programming Arduino and Raspberry Pi boards. Arduino Since the Arduino board is a quite mature technology there are lots of materials covering different aspects of developing programs to control peripheral devices connected to the controller. The development process is trivial: the program are written on a PC or a laptop, compiled and downloaded to the Arudino controller by using a USB cable. Most probably, you will use Arduino IDE to create programs, so it will perform all the actions described above automatically. Before attempts to prepare the first sketch (a sketch is the name that Arduino uses for a program) read few advices that could help to become more successful as an Arduino programmer: Writing code of Arduino devices . 5 tips to improve your Arduino coding skills . If you had no any experience with Arduino, it makes sense to go through the set of lessons to get basic ideas about this device programming. So, do not hesitate and look at Arduino lessons . Different ways to control motors and query sensors has been covered in the section Electromechanical components therefore they will not be repeated here. Instead it is necessary to focus on some tips and tricks, and advanced techniques that are used for complex projects. For example, the portal The Robotics Back-End has the great list of Arduino tutorials. Here you can find an answer for question whether Arduino is good for industrial projects , learn how to write multitasking applications on Arduino , practise to create your own Arduino library and understand techniques to write realtime software for Arduino . Eventually, you could be ready to use the Arduino board as a powerful driver. The main controller (Raspberry Pi) will encode and send commands to this driver, it will decode the commands and schedule getting information from sensors or prepare instructions for motors. Consider the following set of useful tutorials and libraries that could help you build control systems based on Arduino: A library for Non-trivial programing for Arduino Event-based Programming with Arduino State machines with Arduino: State machines tutorial Multitasking on the Arduino with a Finite State Machine Finite State Machine Programming Basics: part 1 and part 2 A library that implements a basic State Machine Separately it is necessary to say about the digital signal processing . When you work with digital and analogue sensors, you will definitely meet with the fact that most of these sensors are noisy. In some cases they could have their own noise suppression mechanism but sometimes it is necessary to implement your own approach to handle data received from the sensors. That's why it makes sense to start learning different ways how to \"de-noise\" the sensor readings, so take into consideration the following articles: Three Methods to Filter Noisy Arduino Measurements Simple High-pass, Band-pass and Band-stop Filtering Implementation of Basic Filters Raspberry Pi One could consider the Raspberry Pi board as a usual computer (PC or laptop) -- a general operating system needs to be installed on the device, it must be equipped with a storage to keep files, a keyboard and a display could be connected to a special sockets. This allows every one to experiment with the board before it will be incorporated in a robotics project. Although Microsoft Windows 10 supports the Raspberry Pi, the common recommendation is to install a Linux-based operating system. Depending on the type of distribution you will choose, low power consumption and high performance could be achieved with a proper configuration. So, in order to getting with the board, at first, start with the installation . Then, especially you did not work before with a Linux systems, take a tour what is available on the Raspberry Pi to be used on daily basis and dig into Linux essentials to learn more about commands that could help you to use the system more effectively. As soon as the first issues were solved and there is an understanding how turn on the board, start using it and gracefully switch off, the next step will be to start programming it. Even if it is possible to program on any language you would like, the most convenient way will be to use Python. This programming languages has low entrance level and have tons of materials in the Internet to begin coding with it. For example, you can try LearnPython.org . If the first programming concepts are clear, the logical continuation is to understand how the language can be used to interact with electronic components . And, finally, you would like to start using the Raspberry Pi together with a camera. Here it is necessary to say, that the board supports several options to work with camera. First of all, the same as for the general purpose computer, the camera can be plugged into an USB socket. In the python code you need to get access a proper device (usually, /dev/video0 ) and start reading frames from it: #!/usr/bin/python import os import pygame, sys from pygame.locals import * import pygame.camera width = 640 height = 480 #initialise pygame pygame.init() pygame.camera.init() cam = pygame.camera.Camera(\"/dev/video0\",(width,height)) cam.start() #setup window windowSurfaceObj = pygame.display.set_mode((width,height),1,16) pygame.display.set_caption('Camera') #take a picture image = cam.get_image() cam.stop() #display the picture catSurfaceObj = image windowSurfaceObj.blit(catSurfaceObj,(0,0)) pygame.display.update() #save picture pygame.image.save(windowSurfaceObj,'picture.jpg') The code is taken from the official forum Another way is to use a specialized Raspberry Pi camera. Since it is connected through a special socket, you can get better results from performance point of view. But from other side, before using the camera's port, it needs to be explicitly turned on in the Raspberry PI Software Configuration tool. You can follow this tutorial that help you to check how the Raspberry Pi camera can be programed. And now you could be ready to start building a self-driven car around the Raspberry Pi! Don't rush -- consider also an approach to connect to the board by a Secure Shell terminal through WiFi . It will be more convenient to run and stop programs remotely without necessity to have keyboard and monitor plugged into a moving car. Another article could be useful for more advanced users -- it brings the light on other aspects of communicating to the Raspberry PI board by using a network connection. At this moment, you can read a short story (with lots of technical details) how to build a Vision-Controlled Car Using Raspberry Pi From Scratch . Since Python is a general purpose language and the Raspberry Pi board runs a general purpose operating system you can implement any method or use any technique to write a control program for your self-driven car. Definitely the program should not be linear since the robot must act adequately when drive through the track and avoid an obstacle. Adequately means in proper time and without blocking the program to react on other event. So, probably the first step to understand how it can be done is to look at two ways to handle control loops in Python .","title":"How to program"},{"location":"p03-programming/#how-to-program","text":"This section provides links on the materials describing different aspects of programming Arduino and Raspberry Pi boards.","title":"How to program"},{"location":"p03-programming/#arduino","text":"Since the Arduino board is a quite mature technology there are lots of materials covering different aspects of developing programs to control peripheral devices connected to the controller. The development process is trivial: the program are written on a PC or a laptop, compiled and downloaded to the Arudino controller by using a USB cable. Most probably, you will use Arduino IDE to create programs, so it will perform all the actions described above automatically. Before attempts to prepare the first sketch (a sketch is the name that Arduino uses for a program) read few advices that could help to become more successful as an Arduino programmer: Writing code of Arduino devices . 5 tips to improve your Arduino coding skills . If you had no any experience with Arduino, it makes sense to go through the set of lessons to get basic ideas about this device programming. So, do not hesitate and look at Arduino lessons . Different ways to control motors and query sensors has been covered in the section Electromechanical components therefore they will not be repeated here. Instead it is necessary to focus on some tips and tricks, and advanced techniques that are used for complex projects. For example, the portal The Robotics Back-End has the great list of Arduino tutorials. Here you can find an answer for question whether Arduino is good for industrial projects , learn how to write multitasking applications on Arduino , practise to create your own Arduino library and understand techniques to write realtime software for Arduino . Eventually, you could be ready to use the Arduino board as a powerful driver. The main controller (Raspberry Pi) will encode and send commands to this driver, it will decode the commands and schedule getting information from sensors or prepare instructions for motors. Consider the following set of useful tutorials and libraries that could help you build control systems based on Arduino: A library for Non-trivial programing for Arduino Event-based Programming with Arduino State machines with Arduino: State machines tutorial Multitasking on the Arduino with a Finite State Machine Finite State Machine Programming Basics: part 1 and part 2 A library that implements a basic State Machine Separately it is necessary to say about the digital signal processing . When you work with digital and analogue sensors, you will definitely meet with the fact that most of these sensors are noisy. In some cases they could have their own noise suppression mechanism but sometimes it is necessary to implement your own approach to handle data received from the sensors. That's why it makes sense to start learning different ways how to \"de-noise\" the sensor readings, so take into consideration the following articles: Three Methods to Filter Noisy Arduino Measurements Simple High-pass, Band-pass and Band-stop Filtering Implementation of Basic Filters","title":"Arduino"},{"location":"p03-programming/#raspberry-pi","text":"One could consider the Raspberry Pi board as a usual computer (PC or laptop) -- a general operating system needs to be installed on the device, it must be equipped with a storage to keep files, a keyboard and a display could be connected to a special sockets. This allows every one to experiment with the board before it will be incorporated in a robotics project. Although Microsoft Windows 10 supports the Raspberry Pi, the common recommendation is to install a Linux-based operating system. Depending on the type of distribution you will choose, low power consumption and high performance could be achieved with a proper configuration. So, in order to getting with the board, at first, start with the installation . Then, especially you did not work before with a Linux systems, take a tour what is available on the Raspberry Pi to be used on daily basis and dig into Linux essentials to learn more about commands that could help you to use the system more effectively. As soon as the first issues were solved and there is an understanding how turn on the board, start using it and gracefully switch off, the next step will be to start programming it. Even if it is possible to program on any language you would like, the most convenient way will be to use Python. This programming languages has low entrance level and have tons of materials in the Internet to begin coding with it. For example, you can try LearnPython.org . If the first programming concepts are clear, the logical continuation is to understand how the language can be used to interact with electronic components . And, finally, you would like to start using the Raspberry Pi together with a camera. Here it is necessary to say, that the board supports several options to work with camera. First of all, the same as for the general purpose computer, the camera can be plugged into an USB socket. In the python code you need to get access a proper device (usually, /dev/video0 ) and start reading frames from it: #!/usr/bin/python import os import pygame, sys from pygame.locals import * import pygame.camera width = 640 height = 480 #initialise pygame pygame.init() pygame.camera.init() cam = pygame.camera.Camera(\"/dev/video0\",(width,height)) cam.start() #setup window windowSurfaceObj = pygame.display.set_mode((width,height),1,16) pygame.display.set_caption('Camera') #take a picture image = cam.get_image() cam.stop() #display the picture catSurfaceObj = image windowSurfaceObj.blit(catSurfaceObj,(0,0)) pygame.display.update() #save picture pygame.image.save(windowSurfaceObj,'picture.jpg') The code is taken from the official forum Another way is to use a specialized Raspberry Pi camera. Since it is connected through a special socket, you can get better results from performance point of view. But from other side, before using the camera's port, it needs to be explicitly turned on in the Raspberry PI Software Configuration tool. You can follow this tutorial that help you to check how the Raspberry Pi camera can be programed. And now you could be ready to start building a self-driven car around the Raspberry Pi! Don't rush -- consider also an approach to connect to the board by a Secure Shell terminal through WiFi . It will be more convenient to run and stop programs remotely without necessity to have keyboard and monitor plugged into a moving car. Another article could be useful for more advanced users -- it brings the light on other aspects of communicating to the Raspberry PI board by using a network connection. At this moment, you can read a short story (with lots of technical details) how to build a Vision-Controlled Car Using Raspberry Pi From Scratch . Since Python is a general purpose language and the Raspberry Pi board runs a general purpose operating system you can implement any method or use any technique to write a control program for your self-driven car. Definitely the program should not be linear since the robot must act adequately when drive through the track and avoid an obstacle. Adequately means in proper time and without blocking the program to react on other event. So, probably the first step to understand how it can be done is to look at two ways to handle control loops in Python .","title":"Raspberry Pi"},{"location":"p04-cv/","text":"Computer vision One of the focus of the Future Engineers competition is to allow students to learn how to use computer vision for solving practical problems. Computer vision is an interdisciplinary scientific field that deals with how computers can gain high-level understanding from digital images or videos. From the perspective of engineering, it seeks to understand and automate tasks that the human visual system can do. This definition is picked up from Wikipedia Although the overall trend in the modern information technologies is to use the computer vision together the machine learning approaches that also include deep learning , the Future Engineers competition does not require to be getting familiar with them. Below you can find clarifications why it is so. Physical considerations Before writing a line of the code for the computer vision implementation, think: what resolution of the camera is suitable for the vehicle how must the camera be directed to solve the task more efficiently whether it is necessary to mount another lens before the camera's image plane Camera resolution The camera resolution determines how many pixels is in the frame received from the camera. Usually it is specified as the amount of pixels in the row and the number of rows. For example, 640 x 480 says that each frame will contain 480 rows as so every row consists of 640 pixels. Totally it will be 307200 pixels in the frame. This could bring you to understanding that higher resolution frames (e.g. standard HD, 720p that is 1280 x 720) contains more pixels than a low resolution frames. So, it is logically that more time is required to transfer these pixels and process them by an image processing algorithm. If your CPU is not so powerful, you would prefer to use cameras with lower resolution or configure a high resolution camera to work in low resolution mode. The rate how fast your algorithm is able to receive and process the frames from camera is characterized by the metric called FPS - frames per second. The value of this metric is very important for the autonomous vehicles: the higher value allows to perceive the road conditions more frequently. So, the response on a sudden change of the conditions (e.g. a new object appears in the camera's field of view) will be shorter. For the conditions like we have on the Future Engineers challenge the higher FPS value allows to increase the speed of the vehicle. The opposite side of cameras with a low resolution is a larger size of the objects that can be distinguished on the resulting frame. Consider the following examples: the first image was made with the resolution 640x480, the second image was with the resolution 160x120. Although the images were made from the same camera position and for the same angle of view, it is clear that the thickness of the blue and orange lines is not enough to recognize them on the second image. Point of view The direction how the camera is fixed on the vehicle determines which objects will appear in the image frame. Obviously, that in order to simplify the image processing we would like to get rid of the elements that are behind of the game field walls like like judges or spectaculars. Here, the camera is fixed on the height of 100 mm and the pitch of the sensor plane is 10 degrees. Static image: Video stream: As you can see on the static image the top edge of the exterior wall is located as so the camera almost does not perceive elements outside of the wall. The video stream is taken with another resolution, that's why the camera gets more information that does not belong to the game field. But pay attention on the average level of the wall's top edge - it does not change when the vehicle is moving, so ROI (region of interest) will help limit the area for the image processing. From other side, the game objects that are in the the frame must not interfere each other. On the picture above the red and green cubes are almost on the same level, that's why, since the vehicle needs to turn left for the green cube and turn right for the right cube, it could be hard to make a decision which direction to choose. In order to get rid of this ambiguity the camera can be fixed on a higher position and/or the pitch could be increased. These samples are for the camera fixed on the height of 175 mm and and the pitch of the sensor plane is 14 degrees. Static image: Video stream: Another pair of samples is for the camera fixed on the height of 280 mm and and the pitch of the sensor plane is 17 degrees. Static image: Video stream: Please note, although the top edge of the exterior wall is located as so the frame does not contain any object outside the game field, the scene changes in the dynamic (on the video streams) represents that it could be hard to specify such ROI when that will reduce number of unnecessary elements but will allow to have early detection of the colored cube at the same time (earlier detection of the element simplifies the motion planning for the robot). Lens One could find that the lens the camera is equipped by default are not suitable for the particular task. This is a list of possible issues with the default lens: - depth of field is to small so the objects distanced from the camera's image plane are too blurred. - the light intensity delivered by the lens to the camera's sensor is not enough so the resulting image is too dark or too noisy - the physical mount of the lens is not rigid as so the camera is unfocused when the vehicle is moving Another possible case that needs to be consider to choose other lens is that by using wide-angle lens the amount of useful information delivered to the camera's sensor can be increased without changing the point of view. Look at the following video streams: The angle of view is 45 degrees: The angle of view is 120 degrees: In both cases the vehicle drove almost the same path, the camera was fixed on the height 100mm and the pitch was 17 mm, but you can see that in the second case the camera perceives more information so the algorithm could discover the walls, perform early detection of the colored cube and track the cube until it is fully passed. Objects recognition A convenient way for beginners to start getting familiar with the computer vision approaches is to work with the OpenCV library . This library has bindings for almost all popular programming languages. Those who use Python and don't familiar with the OpenCV can learn essentials of the library through the official tutorials . Below it will be presented a general pipeline that can be implemented as a base of the algorithm for participation in the Future Engineers competition. Region of interest Consider that there is a following image of two objects: red and green cubes: Imagine the situation when we know in advance a distance which the object should be recognized on. If the object is located on the same plane where the vehicle is moving upon, it means that the bottom edge of the object's shape on the image could be expected approximately on the same rows of pixels. Bases on this fact, we can choose a region of the image to work only with it further. Since the region is less than entire image, the next image processing and an object detection will be performed faster. Cropped image: Basic OpenCV operations with images are covered in this tutorial . Image de-nosing Even if on the image above one object looks red and another one looks green, a small zoom will demonstrate that the colors are not uniform (especially on the green object). Red cube: Green cube: Bearing in mind that the color on the digital images is represented by a set of numbers, if we specify some specific color we cannot expect that a) either this color exists on the image at all; b) or we can find enough amount of pixels of this color to say for sure that there is an object of this color on the image. That's why usually the images are smoothed to reduce gradations of the same color. The same process is used in the digital photography to reduce amount of digital noise on the image. Smoothed image: How to implement smoothing with OpenCV can be read here . Thresholding Thresholding is a way to find such pixels on an image that correspond some properties. Depending on the color scheme used to encode the image the property of the pixel can be its color. For example, the widely used RGB color scheme allows to say how much red, green or blue component is contained in a pixel. The completely white pixel contains maximum of red, green and blue colors. The completely black pixel contains no red, green and blue colors at all. So, someone could assume that if we need to discover a red object on the image, we need to look at the red component of each pixel and if it is maximum the pixel is belonged to a red object. But it is not true. Let's look on the red components of the pixels in the image: Here the most bright pixels correspond to the maximum of the red component. As you can see lots of pixels have maximum in th red component. Why? Recall that the red component of a white pixel is the highest possible. In fact, it is more logical to look at the pixels where the red component is maximum and the blue and green components are on the lowest values. Green component: Blue component: But this leads to nontrivial logic to implement threshold which will become even more complicated if the light condition are being changed. That is why it makes sense to use another color scheme which is more suitable for color recognition. This scheme is called HSV (Hue+Saturation+Value) . The color of the pixel is encoded by the Hue component. The changing light conditions will not affect the Hue component but are reflected in the Saturation and Value components. Here is how the image looks like if HSV components are separated (the most bright pixels correspond to the highest value of the component): Hue component: Saturation component: Value component: So if we know the range that corresponds to some color in the Hue component we can filter out all pixels within this range. The OpenCV library allows to generate a binary representation of the image where the black pixels correspond to the original image's pixels that are outside of the range. This representation is also called the \"mask\". The mask for the red object: The mask for the green object: The example of the filtering out the pixels in the HSV color scheme can be found in this article . In some cases it can be found that even if smoothing was applied to the image before thresholding the result (the mask) is still not solid like the mask for the red object on the image above. In this case, one could consider to use one of the morphological transformations to enhance the mask. For example, if the transformation \"dilation followed by erosion\" (also known as Closing) are applied to the red object's mask, the amount of \"holes\" inside of the mask can be reduced: Note that additional transformation will slow down the overall process of the objects recognition so use it only in case when smoothing plus thresholding do not return good results. More details about the morphological transformations can be found in the corresponding part of the OpenCV documentation . Center of an object (centroid) The useful result of having the mask for an object on the image is that the contour of the object becomes unambiguously detectable. Even if by some reason on the mask there are more than one contour they can be sorted by their areas so the contour with the max area will later considered as the target for the algorithm. And as soon as the contour is known, the pixels within the contour could be used for different purposes. The usage of an object's pixels information to identify the properties of the objects is called the image moment . For example, if you accumulate X coordinates of all pixels belonging to an object and divide the result by the total number of pixels (area) in this object, you will get an average X coordinate. If you do the same for the Y coordinates, eventually you will get the center of the object (centroid). The Y coordinate of the red object centroid: The Y coordinate of the green object centroid: The Y coordinate of the centroid is useful to track the position of the object with respect to the vehicle in the Future Engineers competition. If the object is green and its Y coordinate is in the left side of the frame, the vehicle must perform a maneuver to the left as so the centroid of the green object is measured closer to the right side of the next frame. The following tutorials will help to understand how contours could be detected with the OpenCV library and how center of the objects can be received from the contours: OpenCV center of contour Ball Tracking with OpenCV There is a big section in the OpenCV documentation dedicated for the image contours and operations with them: Lane detection Some of techniques described above can be used to detect the black walls surrounding the track. It means that the vehicle can use the camera not only to detect the green and red objects but also to detect the lane and adjust the steering mechanism correspondingly to drive in the middle of the lane. Here are two articles that cover in details how to implement either the lane following (but when the lane is marked by the tape) or the line following . These articles are also useful since demonstrate how to solve the task on the hardware that is similar to the equipment allowed to be used in the Future Engineers competition. Performance improvements The operation to read a frame from a camera could quite costly from performance perspective as per the nature of the I/O (input/output) layer implementation. Imagine, when the frame is being received by the OpenCV library from the camera the rest of your code just waits. That is why it seems reasonable to divide the program at least on two parts working in parallel: one will read new frames, another will process appearing frames. Such parts of the program are called threads. And this tutorial explains how the performance of the image processing program can be improved with usage of threads . Intellectual cameras Another approach to allow your code performs only \"useful\" action is to delegate the frames collection and, probably, some initial processing to a separate controller. That is why the idea to use intellectual cameras that could be considered as combination of such controller plus a camera looks smart enough. Here is links to the wiki pages of PixyCam v2 and the documentation for OpenMVCam .","title":"Computer vision"},{"location":"p04-cv/#computer-vision","text":"One of the focus of the Future Engineers competition is to allow students to learn how to use computer vision for solving practical problems. Computer vision is an interdisciplinary scientific field that deals with how computers can gain high-level understanding from digital images or videos. From the perspective of engineering, it seeks to understand and automate tasks that the human visual system can do. This definition is picked up from Wikipedia Although the overall trend in the modern information technologies is to use the computer vision together the machine learning approaches that also include deep learning , the Future Engineers competition does not require to be getting familiar with them. Below you can find clarifications why it is so.","title":"Computer vision"},{"location":"p04-cv/#physical-considerations","text":"Before writing a line of the code for the computer vision implementation, think: what resolution of the camera is suitable for the vehicle how must the camera be directed to solve the task more efficiently whether it is necessary to mount another lens before the camera's image plane","title":"Physical considerations"},{"location":"p04-cv/#camera-resolution","text":"The camera resolution determines how many pixels is in the frame received from the camera. Usually it is specified as the amount of pixels in the row and the number of rows. For example, 640 x 480 says that each frame will contain 480 rows as so every row consists of 640 pixels. Totally it will be 307200 pixels in the frame. This could bring you to understanding that higher resolution frames (e.g. standard HD, 720p that is 1280 x 720) contains more pixels than a low resolution frames. So, it is logically that more time is required to transfer these pixels and process them by an image processing algorithm. If your CPU is not so powerful, you would prefer to use cameras with lower resolution or configure a high resolution camera to work in low resolution mode. The rate how fast your algorithm is able to receive and process the frames from camera is characterized by the metric called FPS - frames per second. The value of this metric is very important for the autonomous vehicles: the higher value allows to perceive the road conditions more frequently. So, the response on a sudden change of the conditions (e.g. a new object appears in the camera's field of view) will be shorter. For the conditions like we have on the Future Engineers challenge the higher FPS value allows to increase the speed of the vehicle. The opposite side of cameras with a low resolution is a larger size of the objects that can be distinguished on the resulting frame. Consider the following examples: the first image was made with the resolution 640x480, the second image was with the resolution 160x120. Although the images were made from the same camera position and for the same angle of view, it is clear that the thickness of the blue and orange lines is not enough to recognize them on the second image.","title":"Camera resolution"},{"location":"p04-cv/#point-of-view","text":"The direction how the camera is fixed on the vehicle determines which objects will appear in the image frame. Obviously, that in order to simplify the image processing we would like to get rid of the elements that are behind of the game field walls like like judges or spectaculars. Here, the camera is fixed on the height of 100 mm and the pitch of the sensor plane is 10 degrees. Static image: Video stream: As you can see on the static image the top edge of the exterior wall is located as so the camera almost does not perceive elements outside of the wall. The video stream is taken with another resolution, that's why the camera gets more information that does not belong to the game field. But pay attention on the average level of the wall's top edge - it does not change when the vehicle is moving, so ROI (region of interest) will help limit the area for the image processing. From other side, the game objects that are in the the frame must not interfere each other. On the picture above the red and green cubes are almost on the same level, that's why, since the vehicle needs to turn left for the green cube and turn right for the right cube, it could be hard to make a decision which direction to choose. In order to get rid of this ambiguity the camera can be fixed on a higher position and/or the pitch could be increased. These samples are for the camera fixed on the height of 175 mm and and the pitch of the sensor plane is 14 degrees. Static image: Video stream: Another pair of samples is for the camera fixed on the height of 280 mm and and the pitch of the sensor plane is 17 degrees. Static image: Video stream: Please note, although the top edge of the exterior wall is located as so the frame does not contain any object outside the game field, the scene changes in the dynamic (on the video streams) represents that it could be hard to specify such ROI when that will reduce number of unnecessary elements but will allow to have early detection of the colored cube at the same time (earlier detection of the element simplifies the motion planning for the robot).","title":"Point of view"},{"location":"p04-cv/#lens","text":"One could find that the lens the camera is equipped by default are not suitable for the particular task. This is a list of possible issues with the default lens: - depth of field is to small so the objects distanced from the camera's image plane are too blurred. - the light intensity delivered by the lens to the camera's sensor is not enough so the resulting image is too dark or too noisy - the physical mount of the lens is not rigid as so the camera is unfocused when the vehicle is moving Another possible case that needs to be consider to choose other lens is that by using wide-angle lens the amount of useful information delivered to the camera's sensor can be increased without changing the point of view. Look at the following video streams: The angle of view is 45 degrees: The angle of view is 120 degrees: In both cases the vehicle drove almost the same path, the camera was fixed on the height 100mm and the pitch was 17 mm, but you can see that in the second case the camera perceives more information so the algorithm could discover the walls, perform early detection of the colored cube and track the cube until it is fully passed.","title":"Lens"},{"location":"p04-cv/#objects-recognition","text":"A convenient way for beginners to start getting familiar with the computer vision approaches is to work with the OpenCV library . This library has bindings for almost all popular programming languages. Those who use Python and don't familiar with the OpenCV can learn essentials of the library through the official tutorials . Below it will be presented a general pipeline that can be implemented as a base of the algorithm for participation in the Future Engineers competition.","title":"Objects recognition"},{"location":"p04-cv/#region-of-interest","text":"Consider that there is a following image of two objects: red and green cubes: Imagine the situation when we know in advance a distance which the object should be recognized on. If the object is located on the same plane where the vehicle is moving upon, it means that the bottom edge of the object's shape on the image could be expected approximately on the same rows of pixels. Bases on this fact, we can choose a region of the image to work only with it further. Since the region is less than entire image, the next image processing and an object detection will be performed faster. Cropped image: Basic OpenCV operations with images are covered in this tutorial .","title":"Region of interest"},{"location":"p04-cv/#image-de-nosing","text":"Even if on the image above one object looks red and another one looks green, a small zoom will demonstrate that the colors are not uniform (especially on the green object). Red cube: Green cube: Bearing in mind that the color on the digital images is represented by a set of numbers, if we specify some specific color we cannot expect that a) either this color exists on the image at all; b) or we can find enough amount of pixels of this color to say for sure that there is an object of this color on the image. That's why usually the images are smoothed to reduce gradations of the same color. The same process is used in the digital photography to reduce amount of digital noise on the image. Smoothed image: How to implement smoothing with OpenCV can be read here .","title":"Image de-nosing"},{"location":"p04-cv/#thresholding","text":"Thresholding is a way to find such pixels on an image that correspond some properties. Depending on the color scheme used to encode the image the property of the pixel can be its color. For example, the widely used RGB color scheme allows to say how much red, green or blue component is contained in a pixel. The completely white pixel contains maximum of red, green and blue colors. The completely black pixel contains no red, green and blue colors at all. So, someone could assume that if we need to discover a red object on the image, we need to look at the red component of each pixel and if it is maximum the pixel is belonged to a red object. But it is not true. Let's look on the red components of the pixels in the image: Here the most bright pixels correspond to the maximum of the red component. As you can see lots of pixels have maximum in th red component. Why? Recall that the red component of a white pixel is the highest possible. In fact, it is more logical to look at the pixels where the red component is maximum and the blue and green components are on the lowest values. Green component: Blue component: But this leads to nontrivial logic to implement threshold which will become even more complicated if the light condition are being changed. That is why it makes sense to use another color scheme which is more suitable for color recognition. This scheme is called HSV (Hue+Saturation+Value) . The color of the pixel is encoded by the Hue component. The changing light conditions will not affect the Hue component but are reflected in the Saturation and Value components. Here is how the image looks like if HSV components are separated (the most bright pixels correspond to the highest value of the component): Hue component: Saturation component: Value component: So if we know the range that corresponds to some color in the Hue component we can filter out all pixels within this range. The OpenCV library allows to generate a binary representation of the image where the black pixels correspond to the original image's pixels that are outside of the range. This representation is also called the \"mask\". The mask for the red object: The mask for the green object: The example of the filtering out the pixels in the HSV color scheme can be found in this article . In some cases it can be found that even if smoothing was applied to the image before thresholding the result (the mask) is still not solid like the mask for the red object on the image above. In this case, one could consider to use one of the morphological transformations to enhance the mask. For example, if the transformation \"dilation followed by erosion\" (also known as Closing) are applied to the red object's mask, the amount of \"holes\" inside of the mask can be reduced: Note that additional transformation will slow down the overall process of the objects recognition so use it only in case when smoothing plus thresholding do not return good results. More details about the morphological transformations can be found in the corresponding part of the OpenCV documentation .","title":"Thresholding"},{"location":"p04-cv/#center-of-an-object-centroid","text":"The useful result of having the mask for an object on the image is that the contour of the object becomes unambiguously detectable. Even if by some reason on the mask there are more than one contour they can be sorted by their areas so the contour with the max area will later considered as the target for the algorithm. And as soon as the contour is known, the pixels within the contour could be used for different purposes. The usage of an object's pixels information to identify the properties of the objects is called the image moment . For example, if you accumulate X coordinates of all pixels belonging to an object and divide the result by the total number of pixels (area) in this object, you will get an average X coordinate. If you do the same for the Y coordinates, eventually you will get the center of the object (centroid). The Y coordinate of the red object centroid: The Y coordinate of the green object centroid: The Y coordinate of the centroid is useful to track the position of the object with respect to the vehicle in the Future Engineers competition. If the object is green and its Y coordinate is in the left side of the frame, the vehicle must perform a maneuver to the left as so the centroid of the green object is measured closer to the right side of the next frame. The following tutorials will help to understand how contours could be detected with the OpenCV library and how center of the objects can be received from the contours: OpenCV center of contour Ball Tracking with OpenCV There is a big section in the OpenCV documentation dedicated for the image contours and operations with them:","title":"Center of an object (centroid)"},{"location":"p04-cv/#lane-detection","text":"Some of techniques described above can be used to detect the black walls surrounding the track. It means that the vehicle can use the camera not only to detect the green and red objects but also to detect the lane and adjust the steering mechanism correspondingly to drive in the middle of the lane. Here are two articles that cover in details how to implement either the lane following (but when the lane is marked by the tape) or the line following . These articles are also useful since demonstrate how to solve the task on the hardware that is similar to the equipment allowed to be used in the Future Engineers competition.","title":"Lane detection"},{"location":"p04-cv/#performance-improvements","text":"The operation to read a frame from a camera could quite costly from performance perspective as per the nature of the I/O (input/output) layer implementation. Imagine, when the frame is being received by the OpenCV library from the camera the rest of your code just waits. That is why it seems reasonable to divide the program at least on two parts working in parallel: one will read new frames, another will process appearing frames. Such parts of the program are called threads. And this tutorial explains how the performance of the image processing program can be improved with usage of threads .","title":"Performance improvements"},{"location":"p04-cv/#intellectual-cameras","text":"Another approach to allow your code performs only \"useful\" action is to delegate the frames collection and, probably, some initial processing to a separate controller. That is why the idea to use intellectual cameras that could be considered as combination of such controller plus a camera looks smart enough. Here is links to the wiki pages of PixyCam v2 and the documentation for OpenMVCam .","title":"Intellectual cameras"},{"location":"p05-misc/","text":"Miscellaneous This section contains the links on other useful materials that could help with the preparation to the competition (e.g. simulations, prototyping, tactics and strategy). The documentation for the course F1TENTH that teaches the foundation of of autonomy by using a self-driven cars. A tutorial how to build a self-driving RC car using Raspberry Pi and machine learning using Google Colab The documentation of the DonkeyCar project. It can be used to build a preliminary view how a car for the competition could look like. And here is a story interesting from engineering point of view how the first DonkeyCar appeared. The series of blogposts to build and program a self-driving car. An article explaining how to assemble a controller to control autonomously a RC car from a smartphone. The series of the tutorials in German how to assemble a customized vehicle and use the DonkeyCar software to control it. Another article covering how to transfor a toy RC car to be autonomously controlled by the Raspberry Pi board. Detailed description how to assemble a self-driven car from Traxxas LaTrax Rally 1/18 4WD and NVIDIA Jetson Nano. Someone could also find useful to consider a review of hardware costs required for this project in order to see which materials can be used in similar projects. A GitHub repo providing materials to control a RC car autonomously by using Raspberry Pi and Neural Networks.","title":"Miscellaneous"},{"location":"p05-misc/#miscellaneous","text":"This section contains the links on other useful materials that could help with the preparation to the competition (e.g. simulations, prototyping, tactics and strategy). The documentation for the course F1TENTH that teaches the foundation of of autonomy by using a self-driven cars. A tutorial how to build a self-driving RC car using Raspberry Pi and machine learning using Google Colab The documentation of the DonkeyCar project. It can be used to build a preliminary view how a car for the competition could look like. And here is a story interesting from engineering point of view how the first DonkeyCar appeared. The series of blogposts to build and program a self-driving car. An article explaining how to assemble a controller to control autonomously a RC car from a smartphone. The series of the tutorials in German how to assemble a customized vehicle and use the DonkeyCar software to control it. Another article covering how to transfor a toy RC car to be autonomously controlled by the Raspberry Pi board. Detailed description how to assemble a self-driven car from Traxxas LaTrax Rally 1/18 4WD and NVIDIA Jetson Nano. Someone could also find useful to consider a review of hardware costs required for this project in order to see which materials can be used in similar projects. A GitHub repo providing materials to control a RC car autonomously by using Raspberry Pi and Neural Networks.","title":"Miscellaneous"},{"location":"p06-robot-sets/","text":"Robot sets Below is the list of set that one could consider to use either as a base to build a vehicle for the Future Engineers competition or as a model to learn and practise skills to participate in the competition. SunFounder PiCar-V Kit V2 . Pay attention that there are two DC motors driving the vehicle - this is not allowed by the competition rules. LEGO Mindstorms EV3 and Raspbery Pi with a camera. One of the possible ways to connect them is by BlueTooth but don't use this way for the competition. Fischertechnik Robotics Competition Set . It could be hard to build a steering mechanism with this set. But the ROBOTCS TXT Controller supports programming on Python . MATRIX Mini Starter Robot Kit and the Pixy2 camera . LEGO Education SPIKE Prime with OpenMV camera. Here is instructions to connect these devices together . After that it will require to deploy a special library to the OpenMV camera -- see examples here . MakeBlock Ultimate 2.0 Kit and Raspberry Pi with a camera. CViC - a set for participation in the autonomous cars competitions for school students. LEGO EV3 Computer Vision Add-on - an extension kit allowing to connect RaspberryPI and EV3 by UART.","title":"Robot sets"},{"location":"p06-robot-sets/#robot-sets","text":"Below is the list of set that one could consider to use either as a base to build a vehicle for the Future Engineers competition or as a model to learn and practise skills to participate in the competition. SunFounder PiCar-V Kit V2 . Pay attention that there are two DC motors driving the vehicle - this is not allowed by the competition rules. LEGO Mindstorms EV3 and Raspbery Pi with a camera. One of the possible ways to connect them is by BlueTooth but don't use this way for the competition. Fischertechnik Robotics Competition Set . It could be hard to build a steering mechanism with this set. But the ROBOTCS TXT Controller supports programming on Python . MATRIX Mini Starter Robot Kit and the Pixy2 camera . LEGO Education SPIKE Prime with OpenMV camera. Here is instructions to connect these devices together . After that it will require to deploy a special library to the OpenMV camera -- see examples here . MakeBlock Ultimate 2.0 Kit and Raspberry Pi with a camera. CViC - a set for participation in the autonomous cars competitions for school students. LEGO EV3 Computer Vision Add-on - an extension kit allowing to connect RaspberryPI and EV3 by UART.","title":"Robot sets"}]}