{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"WRO Future Engineers Getting Started Starting from the 2020 season World Robot Olympiad is expanding the set of competitions offered for students around the world by adding a new competition: \"Future Engineers\". The WRO Future Engineers category is a self driving car competition. Students ages 15 to 19 will design a model car, equip it with electromechanical components, and program it so it will be able to autonomously drive on a track and avoid obstacles. More details about the competition can be found at the official site of WRO Association . The Future Engineers challenge does not limit the students to any specific hardware or programming language, which is why working with electromechanical components (motors and sensors) and/or with a micro controller will be challenging. Moreover, the most effective way to make a car fully autonomous is to use a camera and computer vision algorithms. This is also a completely new direction in WRO competition, never used before. The goal of this site is to provide materials for an initial understanding of how the self-driving vehicle can be assembled and how it can be programmed to achieve results in the competition. This site covers the following topics: Chassis -- how to assemble a vehicle Electromechanical components -- which electromechanical components and controllers can be used and how to connect them to each other Basics of SBM and SBC programming -- materials describing different aspects of programming Arduino and Raspberry Pi boards Computer vision -- a method that can be used to implement an algorithm to solve the challenge Miscellaneous -- extra materials that do not fit in other sections Robot sets -- robot sets that can be used for a quick start Enjoy, as you acquire new knowledge and skills! This site was generated from the content located in the GitHub repository: https://github.com/World-Robot-Olympiad-Association/future-engineers-gs . If you have any suggestions or find bugs/inconsistencies, please report them in the form of the GitHub issue .","title":"WRO Future Engineers Getting Started"},{"location":"#wro-future-engineers-getting-started","text":"Starting from the 2020 season World Robot Olympiad is expanding the set of competitions offered for students around the world by adding a new competition: \"Future Engineers\". The WRO Future Engineers category is a self driving car competition. Students ages 15 to 19 will design a model car, equip it with electromechanical components, and program it so it will be able to autonomously drive on a track and avoid obstacles. More details about the competition can be found at the official site of WRO Association . The Future Engineers challenge does not limit the students to any specific hardware or programming language, which is why working with electromechanical components (motors and sensors) and/or with a micro controller will be challenging. Moreover, the most effective way to make a car fully autonomous is to use a camera and computer vision algorithms. This is also a completely new direction in WRO competition, never used before. The goal of this site is to provide materials for an initial understanding of how the self-driving vehicle can be assembled and how it can be programmed to achieve results in the competition. This site covers the following topics: Chassis -- how to assemble a vehicle Electromechanical components -- which electromechanical components and controllers can be used and how to connect them to each other Basics of SBM and SBC programming -- materials describing different aspects of programming Arduino and Raspberry Pi boards Computer vision -- a method that can be used to implement an algorithm to solve the challenge Miscellaneous -- extra materials that do not fit in other sections Robot sets -- robot sets that can be used for a quick start Enjoy, as you acquire new knowledge and skills! This site was generated from the content located in the GitHub repository: https://github.com/World-Robot-Olympiad-Association/future-engineers-gs . If you have any suggestions or find bugs/inconsistencies, please report them in the form of the GitHub issue .","title":"WRO Future Engineers Getting Started"},{"location":"p01-chassis/","text":"Chassis This section covers aspects of building the chassis and tires for the vehicle for competition. Steering mechanism Even though the game rules of the Future Engineering competition do not require you to build your own vehicle but allow you to use an RC (remote controlled) toy car as a base, some may find it useful to design everything from the very beginning. It will allow you to get granular control over the hardware, increase efficiency, and reduce errors introduced by third party implementations. The design of the vehicle is determined by which approach is chosen to drive the rear wheels and implement the steering mechanics. Usually, in the most simple case, the rear wheels are fixed on the same axle and rotated by one DC motor. The steering mechanism on the front axle could be built in several ways - the idea is to change the angle of the front wheels, thereby changing direction. Since these concepts are already widely implemented in RC toy cars and RC sport vehicles, it makes sense to research available materials. For example, this video presents one of the ways to build the steering mechanism: More videos referenced in the next list consider other approaches to achieve the same goal: 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 . Once you get a basic understanding of what your vehicle could look like, you can look at the following instructions which could help you to think of the assembling process in more professional manner: Click on the picture or here to download the instructions. Note that some instructions above use two motors to rotate the rear wheels. In some cases it will make the control of the vehicle more complex, since the motors could rotate asynchronously, and that needs to be compensated for in the steering. Tires During experiments you may find that the vehicle's tires do not have enough friction with the field surface, so it makes sense to consider creating your own tires for the wheels. The following links could give you other brilliant ideas for how to make the tires for the vehicle: 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 .","title":"Chassis"},{"location":"p01-chassis/#chassis","text":"This section covers aspects of building the chassis and tires for the vehicle for competition.","title":"Chassis"},{"location":"p01-chassis/#steering-mechanism","text":"Even though the game rules of the Future Engineering competition do not require you to build your own vehicle but allow you to use an RC (remote controlled) toy car as a base, some may find it useful to design everything from the very beginning. It will allow you to get granular control over the hardware, increase efficiency, and reduce errors introduced by third party implementations. The design of the vehicle is determined by which approach is chosen to drive the rear wheels and implement the steering mechanics. Usually, in the most simple case, the rear wheels are fixed on the same axle and rotated by one DC motor. The steering mechanism on the front axle could be built in several ways - the idea is to change the angle of the front wheels, thereby changing direction. Since these concepts are already widely implemented in RC toy cars and RC sport vehicles, it makes sense to research available materials. For example, this video presents one of the ways to build the steering mechanism: More videos referenced in the next list consider other approaches to achieve the same goal: 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 . Once you get a basic understanding of what your vehicle could look like, you can look at the following instructions which could help you to think of the assembling process in more professional manner: Click on the picture or here to download the instructions. Note that some instructions above use two motors to rotate the rear wheels. In some cases it will make the control of the vehicle more complex, since the motors could rotate asynchronously, and that needs to be compensated for in the steering.","title":"Steering mechanism"},{"location":"p01-chassis/#tires","text":"During experiments you may find that the vehicle's tires do not have enough friction with the field surface, so it makes sense to consider creating your own tires for the wheels. The following links could give you other brilliant ideas for how to make the tires for the vehicle: 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 .","title":"Tires"},{"location":"p02-electronics/","text":"Electromechanical components Here are some concepts that will help you understand how to make the vehicle smarter than an RC toy. Design questions The obstacle detection and decision making for the motion planning must be done autonomously by an electronic brain rather than by a human being. That's why you will need to choose the set of controllers and electronic components and design the logic to connect them to each other. These choices will mostly depend on the way that the vehicle will solve the task, but some ideas will be common due to the nature of the challenge. These questions will help you determine which electronic components will be needed in your autonomous car: Will a camera be used to recognize the traffic signs (red and green pillars)? Is it a USB-camera or a native device (like the Raspberry Pi camera)? How fast can the video stream from the camera be handled? Which controller will give you suitable performance? Will the vehicle be controlled by one controller or by a main controller and an auxiliary controller? How will the roles between controllers be distributed: handling of the video stream, direct control of the motors, direct acquisition of sensor readings? Will the capacity (input/output pins) of each controller be enough for this? What kind of connection will there be between the controllers? Will the walls be detected by camera or by distance/proximity sensors? Should a special lens kit be used to make the camera's field of view wider to recognize both traffic signs and walls or will several cameras be used instead? How will the traversed path be measured: by counting numbers of turns with a compass or a gyroscope, by calculating the distance with an encoder(s), by detecting colored lines with light sensors? Will a gyroscope and/or an accelerometer be used to assist in accurate driving, to get rid of excessive steering mechanism shifts? What kind of power sources should be used to supply controllers, motors, and sensors? Should it be one source for all of these, or should there be dedicated sources for controllers and motors? Overview Before you start drawing schemes and purchasing the components, it is worth taking time to review existing solutions and discover which electromechanical components are commonly used for robotics projects where the aim is to create a self-driving vehicle. You may find these links helpful. Descriptions of these projects provide different levels of detail. We list several of them here to demonstrate the diversity of approaches: Is your toy RC car good enough to become an autonomous robotics car? A good start to turn an RC car into an Arduino controlled car An example of equipping an RC car with sensors and an Arduino controller Not sure how to start driving autonomously? Start with these four simple, turtle-like cars based in Arduino: 1 , 2 , 3 , 4 Yet another DIY autonomous vehicle, but pay attention on the powering Vehicle's brain If this is the first time you have worked with Single Board Microcontrollers like Arduino or Single Board Computers like Raspberry Pi, consider spending extra time to understand the difference between them. For example, the goal of this article is to summarize advantages of SBM and SBC with advice on when to use Arduino, when to use Raspberry Pi, and when both are best used together. As you can see, the advantages are clear, and it makes sense to compare the Raspberry Pi with other SBCs: Click on the picture to watch the video Another interesting approach could be to use integrated Computer Vision boards instead of SBC. This material compares OpenMV, Pixy, and Jevois. Some may decide to build the vehicle with two controllers: the primary controller is to execute navigation and path planning, and to work with a camera. The secondary controller is to request data from sensors and/or control motors. Most likely, the secondary controller will be an Arduino based controller, and the following step will help you learn how it could be connected to the primary controller. There are several protocols that can be used for this and this document is a good way to start getting familiar with them. Depending on the protocol, the following materials will help you implement a communication channel between Raspberry Pi and Arduino: Serial communication: 1 , 2 , 3 , 4 . I2C communication: 1 , 2 , 3 . Motors The minimal number of motors that are required for a vehicle participating in the Future Engineers competition is two: a driving motor and a motor to steer. Motor Basics Motor Driver Using a Servo Motor for the Steering The driving motor makes the car move. This motor consumes more power than can be provided either by Arduino or by Raspberry Pi, therefore, an additional motor driver circuit is used. Here are examples of how to control DC motors from Arudino: 1 , 2 , 3 ; or from Raspberry Pi: 1 . In order to better understand how to control the speed of a DC motor, it makes sense to look at this article . As an alternative to the DC motors, one could consider using a stepper motor or a continuous servomotor . A servo motor is the most suitable way to change the direction of the vehicle since it provides precise control of the steering process. Here are examples of how to control DC motors from Arudino: 1 , 2 , 3 , 4 ; or from Raspberry Pi: 1 . Encoders It is important to control the driving motor speed and the distance the vehicle is moving. One of the simplest devices for this is an encoder: Click on the picture to watch the video These three articles will help you better understand how encoders work: 1 , 2 , 3 . And here are practical examples of how an encoder can be added to the autonomous vehicle: Setup an encoder on a chassis Use an encoder built in the motor Distance sensors Distance and proximity sensors are commonly used in robotics. For the autonomous vehicle participating in the Future Engineers competition, these kinds of devices will allow you to control the distance to the walls, so that the car will move in the middle, between them. Here is a set of short reviews of different sensors (ultrasonic, infrared, and LiDAR) describing the principles - how they work and which advantages and disadvantages they have: 1 , 2 , 3 . IMU IMU is an abbreviation for the term Inertial Measurement Unit. Usually, this abbreviation is used for a device that combines an accelerometer, gyroscope, and magnetometer. In order to get useful data from this sensor, implementing concepts from linear algebra and calculus is usually required. But depending on the libraries provided by the sensor manufactures and/or open source community, this could already be simplified so that the IMU sensor can be used to measure the path driven by the vehicle and to control steering smoothness. We provide links to three articles that are a good start toward understand how an IMU can be used in robotics projects: 1 , 2 , 3 . Power Powering the self-driving car for the robotics competition is not a trivial task. Depending on the types of motors and the number of controllers, it could be that separate power sources must be provided for different systems. The materials below should help you to better understand approaches that can be used to power the controllers and motors: How to power the Arduino board: 1 , 2 How to power Raspberry Pi Powering Motors with Arduino Important note about ground sharing Using a power bank with Arduino and Motors Additional info to power the project","title":"Electronics"},{"location":"p02-electronics/#electromechanical-components","text":"Here are some concepts that will help you understand how to make the vehicle smarter than an RC toy.","title":"Electromechanical components"},{"location":"p02-electronics/#design-questions","text":"The obstacle detection and decision making for the motion planning must be done autonomously by an electronic brain rather than by a human being. That's why you will need to choose the set of controllers and electronic components and design the logic to connect them to each other. These choices will mostly depend on the way that the vehicle will solve the task, but some ideas will be common due to the nature of the challenge. These questions will help you determine which electronic components will be needed in your autonomous car: Will a camera be used to recognize the traffic signs (red and green pillars)? Is it a USB-camera or a native device (like the Raspberry Pi camera)? How fast can the video stream from the camera be handled? Which controller will give you suitable performance? Will the vehicle be controlled by one controller or by a main controller and an auxiliary controller? How will the roles between controllers be distributed: handling of the video stream, direct control of the motors, direct acquisition of sensor readings? Will the capacity (input/output pins) of each controller be enough for this? What kind of connection will there be between the controllers? Will the walls be detected by camera or by distance/proximity sensors? Should a special lens kit be used to make the camera's field of view wider to recognize both traffic signs and walls or will several cameras be used instead? How will the traversed path be measured: by counting numbers of turns with a compass or a gyroscope, by calculating the distance with an encoder(s), by detecting colored lines with light sensors? Will a gyroscope and/or an accelerometer be used to assist in accurate driving, to get rid of excessive steering mechanism shifts? What kind of power sources should be used to supply controllers, motors, and sensors? Should it be one source for all of these, or should there be dedicated sources for controllers and motors?","title":"Design questions"},{"location":"p02-electronics/#overview","text":"Before you start drawing schemes and purchasing the components, it is worth taking time to review existing solutions and discover which electromechanical components are commonly used for robotics projects where the aim is to create a self-driving vehicle. You may find these links helpful. Descriptions of these projects provide different levels of detail. We list several of them here to demonstrate the diversity of approaches: Is your toy RC car good enough to become an autonomous robotics car? A good start to turn an RC car into an Arduino controlled car An example of equipping an RC car with sensors and an Arduino controller Not sure how to start driving autonomously? Start with these four simple, turtle-like cars based in Arduino: 1 , 2 , 3 , 4 Yet another DIY autonomous vehicle, but pay attention on the powering","title":"Overview"},{"location":"p02-electronics/#vehicles-brain","text":"If this is the first time you have worked with Single Board Microcontrollers like Arduino or Single Board Computers like Raspberry Pi, consider spending extra time to understand the difference between them. For example, the goal of this article is to summarize advantages of SBM and SBC with advice on when to use Arduino, when to use Raspberry Pi, and when both are best used together. As you can see, the advantages are clear, and it makes sense to compare the Raspberry Pi with other SBCs: Click on the picture to watch the video Another interesting approach could be to use integrated Computer Vision boards instead of SBC. This material compares OpenMV, Pixy, and Jevois. Some may decide to build the vehicle with two controllers: the primary controller is to execute navigation and path planning, and to work with a camera. The secondary controller is to request data from sensors and/or control motors. Most likely, the secondary controller will be an Arduino based controller, and the following step will help you learn how it could be connected to the primary controller. There are several protocols that can be used for this and this document is a good way to start getting familiar with them. Depending on the protocol, the following materials will help you implement a communication channel between Raspberry Pi and Arduino: Serial communication: 1 , 2 , 3 , 4 . I2C communication: 1 , 2 , 3 .","title":"Vehicle's brain"},{"location":"p02-electronics/#motors","text":"The minimal number of motors that are required for a vehicle participating in the Future Engineers competition is two: a driving motor and a motor to steer. Motor Basics Motor Driver Using a Servo Motor for the Steering The driving motor makes the car move. This motor consumes more power than can be provided either by Arduino or by Raspberry Pi, therefore, an additional motor driver circuit is used. Here are examples of how to control DC motors from Arudino: 1 , 2 , 3 ; or from Raspberry Pi: 1 . In order to better understand how to control the speed of a DC motor, it makes sense to look at this article . As an alternative to the DC motors, one could consider using a stepper motor or a continuous servomotor . A servo motor is the most suitable way to change the direction of the vehicle since it provides precise control of the steering process. Here are examples of how to control DC motors from Arudino: 1 , 2 , 3 , 4 ; or from Raspberry Pi: 1 .","title":"Motors"},{"location":"p02-electronics/#encoders","text":"It is important to control the driving motor speed and the distance the vehicle is moving. One of the simplest devices for this is an encoder: Click on the picture to watch the video These three articles will help you better understand how encoders work: 1 , 2 , 3 . And here are practical examples of how an encoder can be added to the autonomous vehicle: Setup an encoder on a chassis Use an encoder built in the motor","title":"Encoders"},{"location":"p02-electronics/#distance-sensors","text":"Distance and proximity sensors are commonly used in robotics. For the autonomous vehicle participating in the Future Engineers competition, these kinds of devices will allow you to control the distance to the walls, so that the car will move in the middle, between them. Here is a set of short reviews of different sensors (ultrasonic, infrared, and LiDAR) describing the principles - how they work and which advantages and disadvantages they have: 1 , 2 , 3 .","title":"Distance sensors"},{"location":"p02-electronics/#imu","text":"IMU is an abbreviation for the term Inertial Measurement Unit. Usually, this abbreviation is used for a device that combines an accelerometer, gyroscope, and magnetometer. In order to get useful data from this sensor, implementing concepts from linear algebra and calculus is usually required. But depending on the libraries provided by the sensor manufactures and/or open source community, this could already be simplified so that the IMU sensor can be used to measure the path driven by the vehicle and to control steering smoothness. We provide links to three articles that are a good start toward understand how an IMU can be used in robotics projects: 1 , 2 , 3 .","title":"IMU"},{"location":"p02-electronics/#power","text":"Powering the self-driving car for the robotics competition is not a trivial task. Depending on the types of motors and the number of controllers, it could be that separate power sources must be provided for different systems. The materials below should help you to better understand approaches that can be used to power the controllers and motors: How to power the Arduino board: 1 , 2 How to power Raspberry Pi Powering Motors with Arduino Important note about ground sharing Using a power bank with Arduino and Motors Additional info to power the project","title":"Power"},{"location":"p03-programming/","text":"How to program This section provides links to materials describing different aspects of programming Arduino and Raspberry Pi boards. Arduino Since the Arduino board is a mature technology, there are lots of materials covering different aspects of developing programs to control peripheral devices connected to the controller. The development process is simple: the programs are written on a PC or a laptop, them compiled and downloaded to the Arudino controller by using a USB cable. Most likely, you will use Arduino IDE to create programs, so it will perform all the actions described above automatically. Before you attempt to prepare the first sketch (a sketch is the name that Arduino uses for a program) read few articles that could help you become more successful as an Arduino programmer: Writing code for Arduino devices . 5 tips to improve your Arduino coding skills . If you have not had any experience with Arduino, it makes sense to go through a set of lessons to get basic concepts about programming this device. So do not hesitate to look at Arduino lessons . Different ways to control motors and query sensors are covered in the section Electromechanical components , therefore they will not be repeated here. Instead, it is necessary to focus on some tips and tricks and advanced techniques that are used for complex projects. For example, the portal The Robotics Back-End has a great list of Arduino tutorials. Here you can find an answer for the question of whether Arduino is good for industrial projects , learn how to write multitasking applications on Arduino , practise to create your own Arduino library and understand techniques to write realtime software for Arduino . Eventually, you will be ready to use the Arduino board as a powerful driver. The main controller (Raspberry Pi) will encode and send commands to this driver. It will decode the commands and schedule retrieving information from sensors or prepare instructions for motors. Consider the following set of useful tutorials and libraries that could help you build control systems based on Arduino: A library for Non-trivial programing for Arduino Event-based Programming with Arduino State machines with Arduino: State machines tutorial Multitasking on the Arduino with a Finite State Machine Finite State Machine Programming Basics: part 1 and part 2 A library that implements a basic State Machine It is also necessary to mention the digital signal processing . When you work with digital and analogue sensors, you will definitely discover the fact that most of these sensors are noisy. In some cases, they could have their own noise suppression mechanism, but sometimes it is necessary to implement your own approach to handle data received from the sensors. That's why it makes sense to start learning different ways to \"de-noise\" the sensor readings. So take into consideration the following articles: Three Methods to Filter Noisy Arduino Measurements Simple High-pass, Band-pass and Band-stop Filtering Implementation of Basic Filters Raspberry Pi One could consider the Raspberry Pi board as a usual computer (PC or laptop) -- a general operating system needs to be installed on the device, it must be equipped with storage to keep files, a keyboard and a display could be connected to special sockets. This allows you to experiment with the board before it is incorporated into a robotics project. Although Microsoft Windows 10 supports the Raspberry Pi, the common recommendation is to install a Linux-based operating system. Depending on the type of distribution you choose, low power consumption and high performance can be achieved with a proper configuration. In order to interact with the board, first start with the installation . Then, especially if you have not worked with a Linux operating system before, take a tour what is available on the Raspberry Pi for use on a daily basis, and dig into Linux essentials to learn more about commands that could help you use the system more effectively. As soon as the first lessons are complete and you have an understanding of how turn on the board, start using it, and gracefully switch off, the next step will be to start programming it. Even if it is possible to program in any language you would like, the most convenient way will be to use Python. This programming language has a low entrance level and has tons of materials on the Internet to begin coding with it. For example, you can try LearnPython.org . If the first programming concepts are clear, the logical continuation is to understand how the language can be used to interact with electronic components . And, finally, you would like to start using the Raspberry Pi together with a camera. The Raspberry Pi board supports several options to work with camera. First, as with the general purpose computer, the camera can be plugged into a USB socket. In the python code, you need to get access to a proper device (usually, /dev/video0 ) and start reading frames from it: #!/usr/bin/python import os import pygame, sys from pygame.locals import * import pygame.camera width = 640 height = 480 #initialise pygame pygame.init() pygame.camera.init() cam = pygame.camera.Camera(\"/dev/video0\",(width,height)) cam.start() #setup window windowSurfaceObj = pygame.display.set_mode((width,height),1,16) pygame.display.set_caption('Camera') #take a picture image = cam.get_image() cam.stop() #display the picture catSurfaceObj = image windowSurfaceObj.blit(catSurfaceObj,(0,0)) pygame.display.update() #save picture pygame.image.save(windowSurfaceObj,'picture.jpg') The code is taken from the official forum Another way is to use a specialized Raspberry Pi camera. Since it is connected through a special socket, you can get better results from a performance point of view. On the other hand, before using the camera's port, it needs to be explicitly turned on in the Raspberry PI Software Configuration tool. You can follow this tutorial that helps you learn how the Raspberry Pi camera can be programmed. And now you could be ready to start building a self-driven car around the Raspberry Pi! Don't rush -- consider an approach to connect to the board by a Secure Shell terminal through WiFi . It will be more convenient to run and stop programs remotely without the necessity of having a keyboard and monitor plugged into a moving car. Another article could be useful for more advanced users -- it sheds light on other aspects of communicating to the Raspberry PI board by using a network connection. Take a moment to read a short story (with lots of technical details) on how to build a Vision-Controlled Car Using Raspberry Pi From Scratch . Since Python is a general purpose language and the Raspberry Pi board runs a general purpose operating system, you can implement any method or use any technique to write a control program for your self-driven car. Definitely, the program should not be linear, since the robot must act adequately when driving through the track and avoiding an obstacle. Adequately means in proper time and without preventing the program from reacting to other events. The first step to understand how it can be done is to look at two ways to handle control loops in Python .","title":"How to program"},{"location":"p03-programming/#how-to-program","text":"This section provides links to materials describing different aspects of programming Arduino and Raspberry Pi boards.","title":"How to program"},{"location":"p03-programming/#arduino","text":"Since the Arduino board is a mature technology, there are lots of materials covering different aspects of developing programs to control peripheral devices connected to the controller. The development process is simple: the programs are written on a PC or a laptop, them compiled and downloaded to the Arudino controller by using a USB cable. Most likely, you will use Arduino IDE to create programs, so it will perform all the actions described above automatically. Before you attempt to prepare the first sketch (a sketch is the name that Arduino uses for a program) read few articles that could help you become more successful as an Arduino programmer: Writing code for Arduino devices . 5 tips to improve your Arduino coding skills . If you have not had any experience with Arduino, it makes sense to go through a set of lessons to get basic concepts about programming this device. So do not hesitate to look at Arduino lessons . Different ways to control motors and query sensors are covered in the section Electromechanical components , therefore they will not be repeated here. Instead, it is necessary to focus on some tips and tricks and advanced techniques that are used for complex projects. For example, the portal The Robotics Back-End has a great list of Arduino tutorials. Here you can find an answer for the question of whether Arduino is good for industrial projects , learn how to write multitasking applications on Arduino , practise to create your own Arduino library and understand techniques to write realtime software for Arduino . Eventually, you will be ready to use the Arduino board as a powerful driver. The main controller (Raspberry Pi) will encode and send commands to this driver. It will decode the commands and schedule retrieving information from sensors or prepare instructions for motors. Consider the following set of useful tutorials and libraries that could help you build control systems based on Arduino: A library for Non-trivial programing for Arduino Event-based Programming with Arduino State machines with Arduino: State machines tutorial Multitasking on the Arduino with a Finite State Machine Finite State Machine Programming Basics: part 1 and part 2 A library that implements a basic State Machine It is also necessary to mention the digital signal processing . When you work with digital and analogue sensors, you will definitely discover the fact that most of these sensors are noisy. In some cases, they could have their own noise suppression mechanism, but sometimes it is necessary to implement your own approach to handle data received from the sensors. That's why it makes sense to start learning different ways to \"de-noise\" the sensor readings. So take into consideration the following articles: Three Methods to Filter Noisy Arduino Measurements Simple High-pass, Band-pass and Band-stop Filtering Implementation of Basic Filters","title":"Arduino"},{"location":"p03-programming/#raspberry-pi","text":"One could consider the Raspberry Pi board as a usual computer (PC or laptop) -- a general operating system needs to be installed on the device, it must be equipped with storage to keep files, a keyboard and a display could be connected to special sockets. This allows you to experiment with the board before it is incorporated into a robotics project. Although Microsoft Windows 10 supports the Raspberry Pi, the common recommendation is to install a Linux-based operating system. Depending on the type of distribution you choose, low power consumption and high performance can be achieved with a proper configuration. In order to interact with the board, first start with the installation . Then, especially if you have not worked with a Linux operating system before, take a tour what is available on the Raspberry Pi for use on a daily basis, and dig into Linux essentials to learn more about commands that could help you use the system more effectively. As soon as the first lessons are complete and you have an understanding of how turn on the board, start using it, and gracefully switch off, the next step will be to start programming it. Even if it is possible to program in any language you would like, the most convenient way will be to use Python. This programming language has a low entrance level and has tons of materials on the Internet to begin coding with it. For example, you can try LearnPython.org . If the first programming concepts are clear, the logical continuation is to understand how the language can be used to interact with electronic components . And, finally, you would like to start using the Raspberry Pi together with a camera. The Raspberry Pi board supports several options to work with camera. First, as with the general purpose computer, the camera can be plugged into a USB socket. In the python code, you need to get access to a proper device (usually, /dev/video0 ) and start reading frames from it: #!/usr/bin/python import os import pygame, sys from pygame.locals import * import pygame.camera width = 640 height = 480 #initialise pygame pygame.init() pygame.camera.init() cam = pygame.camera.Camera(\"/dev/video0\",(width,height)) cam.start() #setup window windowSurfaceObj = pygame.display.set_mode((width,height),1,16) pygame.display.set_caption('Camera') #take a picture image = cam.get_image() cam.stop() #display the picture catSurfaceObj = image windowSurfaceObj.blit(catSurfaceObj,(0,0)) pygame.display.update() #save picture pygame.image.save(windowSurfaceObj,'picture.jpg') The code is taken from the official forum Another way is to use a specialized Raspberry Pi camera. Since it is connected through a special socket, you can get better results from a performance point of view. On the other hand, before using the camera's port, it needs to be explicitly turned on in the Raspberry PI Software Configuration tool. You can follow this tutorial that helps you learn how the Raspberry Pi camera can be programmed. And now you could be ready to start building a self-driven car around the Raspberry Pi! Don't rush -- consider an approach to connect to the board by a Secure Shell terminal through WiFi . It will be more convenient to run and stop programs remotely without the necessity of having a keyboard and monitor plugged into a moving car. Another article could be useful for more advanced users -- it sheds light on other aspects of communicating to the Raspberry PI board by using a network connection. Take a moment to read a short story (with lots of technical details) on how to build a Vision-Controlled Car Using Raspberry Pi From Scratch . Since Python is a general purpose language and the Raspberry Pi board runs a general purpose operating system, you can implement any method or use any technique to write a control program for your self-driven car. Definitely, the program should not be linear, since the robot must act adequately when driving through the track and avoiding an obstacle. Adequately means in proper time and without preventing the program from reacting to other events. The first step to understand how it can be done is to look at two ways to handle control loops in Python .","title":"Raspberry Pi"},{"location":"p04-cv/","text":"Computer vision One of the focuses of the Future Engineers competition is to allow students to learn how to use computer vision for solving practical problems. Computer vision is an interdisciplinary scientific field that deals with how computers can gain high-level understanding from digital images or videos. From the perspective of engineering, it seeks to understand and automate tasks that the human visual system can do. This definition is picked up from Wikipedia Although the overall trend in the modern information technologies is to use the computer vision together with the machine learning and approaches that also include deep learning , the Future Engineers competition does not require familiarity with them. Below you can find clarification on why it is so. Physical considerations Before writing a line of the code for the computer vision implementation, think: what resolution of the camera is suitable for the vehicle? how must the camera be directed to solve the task more efficiently? is it necessary to mount another lens before the camera's image plane? Camera resolution The camera resolution determines how many pixels are in the frame received from the camera. Usually, it is specified as the amount of pixels in the row and the number of rows. For example, 640 x 480 says that each frame will contain 480 rows and every row consists of 640 pixels. In total, it will be 307200 pixels in the frame. This should help you to understand that higher resolution frames (e.g. standard HD, 720p that is 1280 x 720) contains more pixels than low resolution frames. So it is logical that more time is required to transfer these pixels and process them with an image processing algorithm. If your CPU is not so powerful, you would do better to use cameras with lower resolution or configure a high resolution camera to work in low resolution mode. The rate at which your algorithm is able to receive and process the frames from a camera is characterized by the metric called FPS - frames per second. The value of this metric is very important for the autonomous vehicles: the higher value allows the vehicle to perceive the road conditions more frequently. So the response time on a sudden change of the conditions (e.g. a new object appears in the camera's field of view) will be shorter. For the conditions like we have on the Future Engineers challenge, the higher FPS value allows you to increase the speed of the vehicle. On the other hand, cameras with low resolution allow for a larger size of the objects that can be distinguished on the resulting frame. Consider the following examples: the first image was made with the resolution 640x480, the second image was with the resolution 160x120. Although the images were made from the same camera position and with the same angle of view, it is clear that the thickness of the blue and orange lines is not enough to recognize them on the second image. Point of view The direction in which the camera is fixed on the vehicle determines which objects will appear in the image frame. In order to simplify the image processing, we would like to get rid of the elements that are behind the game field walls, like judges or spectaculars. Here, the camera is fixed at the height of 100 mm, and the pitch of the sensor plane is 10 degrees. Static image: Video stream: As you can see on the static image, the top edge of the exterior wall is located so that the camera almost does not perceive elements outside of the wall. The video stream is taken with another resolution. That's why the camera gets more information that does not belong to the game field. But pay attention to the average level of the wall's top edge - it does not change when the vehicle is moving, so ROI (region of interest) will help limit the area for the image processing. Additionally, the game objects that are in the the frame must not interfere with each other. On the picture above, the red and green cubes are almost on the same level. That's why, since the vehicle needs to turn left for the green cube and turn right for the red cube, it could be hard for the vehicle to make a decision which direction to choose. In order to get rid of this ambiguity, the camera can be fixed on a higher position and/or the pitch could be increased. These samples are for the camera fixed at the height of 175 mm, and the pitch of the sensor plane is 14 degrees. Static image: Video stream: Another pair of samples is for the camera fixed at the height of 280 mm, and the pitch of the sensor plane is 17 degrees. Static image: Video stream: Please note, although the top edge of the exterior wall is located so that the frame does not contain any object outside the game field, the scene changes in the dynamic (on the video streams) representations so that it could be hard to specify a ROI that will reduce the number of unnecessary elements but will allow the vehicle to have early detection of the colored cube at the same time. (Earlier detection of the element simplifies the motion planning for the robot.) Lens It is possible that the lens the camera is equipped with by default is not suitable for the particular task. This is a list of possible issues with the default lens: - depth of field is too small, so the objects distant from the camera's image plane are too blurred. - the light intensity delivered by the lens to the camera's sensor is not enough, so the resulting image is too dark or too noisy - the physical mount of the lens is not rigid enough, so the camera is unfocused when the vehicle is moving Another possible case that needs to be considered twhen choosing another lens is that by using a wide-angle lens , the amount of useful information delivered to the camera's sensor can be increased without changing the point of view. Look at the following video streams: The angle of view is 45 degrees: The angle of view is 120 degrees: In both cases, the vehicle drove almost the same path. The camera was fixed on the height 100mm and the pitch was 17 mm, but you can see that in the second case the camera perceives more information, so the algorithm can discover the walls, perform early detection of the colored cube, and track the cube until it is fully passed. Objects recognition A convenient way for beginners to start getting familiar with the computer vision approaches is to work with the OpenCV library . This library has holdings for almost all popular programming languages. Those who use Python and aren't familiar with the OpenCV can learn essentials of the library through the official tutorials . Below is a general pipeline that can be implemented as a base of the algorithm for participation in the Future Engineers competition. Region of interest Consider the following image of two objects: red and green cubes: Imagine the situation when we know in advance a distance from which the object should be recognized. If the object is located on the same plane the vehicle is moving upon, it means that the bottom edge of the object's shape on the image could be expected approximately on the same row of pixels. Based on this fact, we can choose a region of the image and work further only with it. Since the region is less than entire image, the next image processing and object detection will be performed faster. Cropped image: Basic OpenCV operations with images are covered in this tutorial . Image de-nosing Even if, on the image above, one object looks red and another one looks green, a small zoom will demonstrate that the colors are not uniform (especially on the green object). Red cube: Green cube: Bearing in mind that the colors on the digital images are represented by a set of numbers, if we specify some specific color, we cannot expect that a) this color necessarily exists on the image at all; or that b) we can find enough pixels of this color to say for sure that there is an object of this color on the image. That's why, usually, the images are smoothed to reduce gradations of the same color. The same process is used in digital photography to reduce the amount of digital noise on the image. Smoothed image: How to implement smoothing with OpenCV can be read here . Thresholding Thresholding is a way to find pixels on an image that correspond with some property. Depending on the color scheme used to encode the image, the property of the pixel can be its color. For example, the widely used RGB color scheme allows us to say how much red, green, or blue component is contained in a pixel. The completely white pixel contains a maximum of red, green, and blue colors. The completely black pixel contains no red, green, or blue colors at all. So, one could assume that if we need to discover a red object on the image, we need to look at the red component of each pixel and if it is at maximum, the pixel belongs to a red object. But it is not true. Let's look at the red components of the pixels in the image: Here, the brightest pixels correspond to the maximum of the red component. As you can see, lots of pixels have a maximum in the red component. Why? Recall that the red component of a white pixel is the highest possible. In fact, it is more logical to look at the pixels where the red component is at maximum and the blue and green components are on the lowest values. Green component: Blue component: But this leads to nontrivial logic to implement a threshold which will become even more complicated if the light conditions are being changed. That is why it makes sense to use another color scheme which is more suitable for color recognition. This scheme is called HSV (Hue+Saturation+Value) . The color of the pixel is encoded by the Hue component. The changing light conditions will not affect the Hue component but are reflected in the Saturation and Value components. Here is how the image looks if HSV components are separated (the brightest pixels correspond to the highest value of the component): Hue component: Saturation component: Value component: So if we know the range that corresponds to some color in the Hue component, we can filter out all pixels within this range. The OpenCV library allows us to generate a binary representation of the image where the black pixels correspond to the original image's pixels that are outside of the range. This representation is also called the \"mask\". The mask for the red object: The mask for the green object: The example of filtering out the pixels in the HSV color scheme can be found in this article . In some cases it can be found that even if smoothing was applied to the image before thresholding, the result (the mask) is still not solid like the mask for the red object on the image above. In this case, one could consider using one of the morphological transformations to enhance the mask. For example, if the transformation \"dilation followed by erosion\" (also known as Closing) is applied to the red object's mask, the amount of \"holes\" inside the mask can be reduced: Note that additional transformation will slow down the overall process of the object's recognition, so use it only in a case when smoothing plus thresholding do not return good results. More details about the morphological transformations can be found in the corresponding part of the OpenCV documentation . Center of an object (centroid) The useful result of having the mask for an object on the image is that the contour of the object becomes unambiguously detectable. Even if, for some reason, there is more than one contour on the mask, they can be sorted by their areas so the contour with the maximum area will later be considered the target for the algorithm. And as soon as the contour is known, the pixels within the contour could be used for different purposes. The use of an object's pixel information to identify the properties of the objects is called the image moment . For example, if you accumulate X coordinates of all pixels belonging to an object and divide the result by the total number of pixels (area) in this object, you will get an average X coordinate. If you do the same for the Y coordinates, eventually you will get the center of the object (centroid). The Y coordinate of the red object centroid: The Y coordinate of the green object centroid: The Y coordinate of the centroid is useful to track the position of the object with respect to the vehicle in the Future Engineers competition. If the object is green and its Y coordinate is in the left side of the frame, the vehicle must perform a maneuver to the left so that the centroid of the green object is measured closer to the right side of the next frame. The following tutorials will help you understand how contours could be detected with the OpenCV library and how the center of the objects can be received from the contours: OpenCV center of contour Ball Tracking with OpenCV There is a big section in the OpenCV documentation dedicated to the image contours and operations with them: Lane detection Some of the techniques described above can be used to detect the black walls surrounding the track. It means that the vehicle can use the camera not only to detect the green and red objects but also to detect the lane and adjust the steering mechanism correspondingly to drive in the middle of the lane. Here are two articles that cover in detail how to implement either the lane following (when the lane is marked by the tape) or the line following . These articles are also useful since they demonstrate how to solve the task on hardware that is similar to the equipment allowed to be used in the Future Engineers competition. Performance improvements The operation to read a frame from a camera could be quite costly from a performance perspective, as per the nature of the I/O (input/output) layer implementation. Imagine, when the frame is being received by the OpenCV library from the camera the rest of your code just waits. That is why it seems reasonable to divide the program at least into two parts working in parallel: one will read new frames, another will process appearing frames. Such parts of the program are called threads. And this tutorial explains how the performance of the image processing program can be improved with usage of threads . Smart cameras Another approach to allow your code to perform only \"useful\" action is to delegate the frame collection and, probably, some initial processing to a separate controller. That is why the idea to use smart cameras - cameras that could be considered as a combination of a controller plus a camera - looks like a good idea. Here is links to the wiki pages of PixyCam v2 and the documentation for OpenMVCam .","title":"Computer vision"},{"location":"p04-cv/#computer-vision","text":"One of the focuses of the Future Engineers competition is to allow students to learn how to use computer vision for solving practical problems. Computer vision is an interdisciplinary scientific field that deals with how computers can gain high-level understanding from digital images or videos. From the perspective of engineering, it seeks to understand and automate tasks that the human visual system can do. This definition is picked up from Wikipedia Although the overall trend in the modern information technologies is to use the computer vision together with the machine learning and approaches that also include deep learning , the Future Engineers competition does not require familiarity with them. Below you can find clarification on why it is so.","title":"Computer vision"},{"location":"p04-cv/#physical-considerations","text":"Before writing a line of the code for the computer vision implementation, think: what resolution of the camera is suitable for the vehicle? how must the camera be directed to solve the task more efficiently? is it necessary to mount another lens before the camera's image plane?","title":"Physical considerations"},{"location":"p04-cv/#camera-resolution","text":"The camera resolution determines how many pixels are in the frame received from the camera. Usually, it is specified as the amount of pixels in the row and the number of rows. For example, 640 x 480 says that each frame will contain 480 rows and every row consists of 640 pixels. In total, it will be 307200 pixels in the frame. This should help you to understand that higher resolution frames (e.g. standard HD, 720p that is 1280 x 720) contains more pixels than low resolution frames. So it is logical that more time is required to transfer these pixels and process them with an image processing algorithm. If your CPU is not so powerful, you would do better to use cameras with lower resolution or configure a high resolution camera to work in low resolution mode. The rate at which your algorithm is able to receive and process the frames from a camera is characterized by the metric called FPS - frames per second. The value of this metric is very important for the autonomous vehicles: the higher value allows the vehicle to perceive the road conditions more frequently. So the response time on a sudden change of the conditions (e.g. a new object appears in the camera's field of view) will be shorter. For the conditions like we have on the Future Engineers challenge, the higher FPS value allows you to increase the speed of the vehicle. On the other hand, cameras with low resolution allow for a larger size of the objects that can be distinguished on the resulting frame. Consider the following examples: the first image was made with the resolution 640x480, the second image was with the resolution 160x120. Although the images were made from the same camera position and with the same angle of view, it is clear that the thickness of the blue and orange lines is not enough to recognize them on the second image.","title":"Camera resolution"},{"location":"p04-cv/#point-of-view","text":"The direction in which the camera is fixed on the vehicle determines which objects will appear in the image frame. In order to simplify the image processing, we would like to get rid of the elements that are behind the game field walls, like judges or spectaculars. Here, the camera is fixed at the height of 100 mm, and the pitch of the sensor plane is 10 degrees. Static image: Video stream: As you can see on the static image, the top edge of the exterior wall is located so that the camera almost does not perceive elements outside of the wall. The video stream is taken with another resolution. That's why the camera gets more information that does not belong to the game field. But pay attention to the average level of the wall's top edge - it does not change when the vehicle is moving, so ROI (region of interest) will help limit the area for the image processing. Additionally, the game objects that are in the the frame must not interfere with each other. On the picture above, the red and green cubes are almost on the same level. That's why, since the vehicle needs to turn left for the green cube and turn right for the red cube, it could be hard for the vehicle to make a decision which direction to choose. In order to get rid of this ambiguity, the camera can be fixed on a higher position and/or the pitch could be increased. These samples are for the camera fixed at the height of 175 mm, and the pitch of the sensor plane is 14 degrees. Static image: Video stream: Another pair of samples is for the camera fixed at the height of 280 mm, and the pitch of the sensor plane is 17 degrees. Static image: Video stream: Please note, although the top edge of the exterior wall is located so that the frame does not contain any object outside the game field, the scene changes in the dynamic (on the video streams) representations so that it could be hard to specify a ROI that will reduce the number of unnecessary elements but will allow the vehicle to have early detection of the colored cube at the same time. (Earlier detection of the element simplifies the motion planning for the robot.)","title":"Point of view"},{"location":"p04-cv/#lens","text":"It is possible that the lens the camera is equipped with by default is not suitable for the particular task. This is a list of possible issues with the default lens: - depth of field is too small, so the objects distant from the camera's image plane are too blurred. - the light intensity delivered by the lens to the camera's sensor is not enough, so the resulting image is too dark or too noisy - the physical mount of the lens is not rigid enough, so the camera is unfocused when the vehicle is moving Another possible case that needs to be considered twhen choosing another lens is that by using a wide-angle lens , the amount of useful information delivered to the camera's sensor can be increased without changing the point of view. Look at the following video streams: The angle of view is 45 degrees: The angle of view is 120 degrees: In both cases, the vehicle drove almost the same path. The camera was fixed on the height 100mm and the pitch was 17 mm, but you can see that in the second case the camera perceives more information, so the algorithm can discover the walls, perform early detection of the colored cube, and track the cube until it is fully passed.","title":"Lens"},{"location":"p04-cv/#objects-recognition","text":"A convenient way for beginners to start getting familiar with the computer vision approaches is to work with the OpenCV library . This library has holdings for almost all popular programming languages. Those who use Python and aren't familiar with the OpenCV can learn essentials of the library through the official tutorials . Below is a general pipeline that can be implemented as a base of the algorithm for participation in the Future Engineers competition.","title":"Objects recognition"},{"location":"p04-cv/#region-of-interest","text":"Consider the following image of two objects: red and green cubes: Imagine the situation when we know in advance a distance from which the object should be recognized. If the object is located on the same plane the vehicle is moving upon, it means that the bottom edge of the object's shape on the image could be expected approximately on the same row of pixels. Based on this fact, we can choose a region of the image and work further only with it. Since the region is less than entire image, the next image processing and object detection will be performed faster. Cropped image: Basic OpenCV operations with images are covered in this tutorial .","title":"Region of interest"},{"location":"p04-cv/#image-de-nosing","text":"Even if, on the image above, one object looks red and another one looks green, a small zoom will demonstrate that the colors are not uniform (especially on the green object). Red cube: Green cube: Bearing in mind that the colors on the digital images are represented by a set of numbers, if we specify some specific color, we cannot expect that a) this color necessarily exists on the image at all; or that b) we can find enough pixels of this color to say for sure that there is an object of this color on the image. That's why, usually, the images are smoothed to reduce gradations of the same color. The same process is used in digital photography to reduce the amount of digital noise on the image. Smoothed image: How to implement smoothing with OpenCV can be read here .","title":"Image de-nosing"},{"location":"p04-cv/#thresholding","text":"Thresholding is a way to find pixels on an image that correspond with some property. Depending on the color scheme used to encode the image, the property of the pixel can be its color. For example, the widely used RGB color scheme allows us to say how much red, green, or blue component is contained in a pixel. The completely white pixel contains a maximum of red, green, and blue colors. The completely black pixel contains no red, green, or blue colors at all. So, one could assume that if we need to discover a red object on the image, we need to look at the red component of each pixel and if it is at maximum, the pixel belongs to a red object. But it is not true. Let's look at the red components of the pixels in the image: Here, the brightest pixels correspond to the maximum of the red component. As you can see, lots of pixels have a maximum in the red component. Why? Recall that the red component of a white pixel is the highest possible. In fact, it is more logical to look at the pixels where the red component is at maximum and the blue and green components are on the lowest values. Green component: Blue component: But this leads to nontrivial logic to implement a threshold which will become even more complicated if the light conditions are being changed. That is why it makes sense to use another color scheme which is more suitable for color recognition. This scheme is called HSV (Hue+Saturation+Value) . The color of the pixel is encoded by the Hue component. The changing light conditions will not affect the Hue component but are reflected in the Saturation and Value components. Here is how the image looks if HSV components are separated (the brightest pixels correspond to the highest value of the component): Hue component: Saturation component: Value component: So if we know the range that corresponds to some color in the Hue component, we can filter out all pixels within this range. The OpenCV library allows us to generate a binary representation of the image where the black pixels correspond to the original image's pixels that are outside of the range. This representation is also called the \"mask\". The mask for the red object: The mask for the green object: The example of filtering out the pixels in the HSV color scheme can be found in this article . In some cases it can be found that even if smoothing was applied to the image before thresholding, the result (the mask) is still not solid like the mask for the red object on the image above. In this case, one could consider using one of the morphological transformations to enhance the mask. For example, if the transformation \"dilation followed by erosion\" (also known as Closing) is applied to the red object's mask, the amount of \"holes\" inside the mask can be reduced: Note that additional transformation will slow down the overall process of the object's recognition, so use it only in a case when smoothing plus thresholding do not return good results. More details about the morphological transformations can be found in the corresponding part of the OpenCV documentation .","title":"Thresholding"},{"location":"p04-cv/#center-of-an-object-centroid","text":"The useful result of having the mask for an object on the image is that the contour of the object becomes unambiguously detectable. Even if, for some reason, there is more than one contour on the mask, they can be sorted by their areas so the contour with the maximum area will later be considered the target for the algorithm. And as soon as the contour is known, the pixels within the contour could be used for different purposes. The use of an object's pixel information to identify the properties of the objects is called the image moment . For example, if you accumulate X coordinates of all pixels belonging to an object and divide the result by the total number of pixels (area) in this object, you will get an average X coordinate. If you do the same for the Y coordinates, eventually you will get the center of the object (centroid). The Y coordinate of the red object centroid: The Y coordinate of the green object centroid: The Y coordinate of the centroid is useful to track the position of the object with respect to the vehicle in the Future Engineers competition. If the object is green and its Y coordinate is in the left side of the frame, the vehicle must perform a maneuver to the left so that the centroid of the green object is measured closer to the right side of the next frame. The following tutorials will help you understand how contours could be detected with the OpenCV library and how the center of the objects can be received from the contours: OpenCV center of contour Ball Tracking with OpenCV There is a big section in the OpenCV documentation dedicated to the image contours and operations with them:","title":"Center of an object (centroid)"},{"location":"p04-cv/#lane-detection","text":"Some of the techniques described above can be used to detect the black walls surrounding the track. It means that the vehicle can use the camera not only to detect the green and red objects but also to detect the lane and adjust the steering mechanism correspondingly to drive in the middle of the lane. Here are two articles that cover in detail how to implement either the lane following (when the lane is marked by the tape) or the line following . These articles are also useful since they demonstrate how to solve the task on hardware that is similar to the equipment allowed to be used in the Future Engineers competition.","title":"Lane detection"},{"location":"p04-cv/#performance-improvements","text":"The operation to read a frame from a camera could be quite costly from a performance perspective, as per the nature of the I/O (input/output) layer implementation. Imagine, when the frame is being received by the OpenCV library from the camera the rest of your code just waits. That is why it seems reasonable to divide the program at least into two parts working in parallel: one will read new frames, another will process appearing frames. Such parts of the program are called threads. And this tutorial explains how the performance of the image processing program can be improved with usage of threads .","title":"Performance improvements"},{"location":"p04-cv/#smart-cameras","text":"Another approach to allow your code to perform only \"useful\" action is to delegate the frame collection and, probably, some initial processing to a separate controller. That is why the idea to use smart cameras - cameras that could be considered as a combination of a controller plus a camera - looks like a good idea. Here is links to the wiki pages of PixyCam v2 and the documentation for OpenMVCam .","title":"Smart cameras"},{"location":"p05-misc/","text":"Miscellaneous This section contains the links to other useful materials that could help with the preparation to the competition (e.g. simulations, prototyping, tactics and strategy). The documentation for the course F1TENTH that teaches the foundations of autonomy by using self-driven cars. A tutorial how to build a self-driving RC car using Raspberry Pi and machine learning using Google Colab The documentation of the DonkeyCar project. It can be used to build a preliminary viewof how a car for the competition could look. And here is a story, interesting from an engineering point of view, of how the first DonkeyCar appeared. The series of blogposts to build and program a self-driving car. An article explaining how to assemble a controller to autonomously control an RC car from a smartphone. The series of the tutorials in German on how to assemble a customized vehicle and use the DonkeyCar software to control it. Another article covering how to transform a toy RC car to be autonomously controlled by the Raspberry Pi board. Detailed description how to assemble a self-driven car from Traxxas LaTrax Rally 1/18 4WD and NVIDIA Jetson Nano. Someone could also find it useful to consider a review of hardware costs required for this project in order to see which materials can be used in similar projects. A GitHub repo providing materials to control an RC car autonomously by using Raspberry Pi and Neural Networks.","title":"Miscellaneous"},{"location":"p05-misc/#miscellaneous","text":"This section contains the links to other useful materials that could help with the preparation to the competition (e.g. simulations, prototyping, tactics and strategy). The documentation for the course F1TENTH that teaches the foundations of autonomy by using self-driven cars. A tutorial how to build a self-driving RC car using Raspberry Pi and machine learning using Google Colab The documentation of the DonkeyCar project. It can be used to build a preliminary viewof how a car for the competition could look. And here is a story, interesting from an engineering point of view, of how the first DonkeyCar appeared. The series of blogposts to build and program a self-driving car. An article explaining how to assemble a controller to autonomously control an RC car from a smartphone. The series of the tutorials in German on how to assemble a customized vehicle and use the DonkeyCar software to control it. Another article covering how to transform a toy RC car to be autonomously controlled by the Raspberry Pi board. Detailed description how to assemble a self-driven car from Traxxas LaTrax Rally 1/18 4WD and NVIDIA Jetson Nano. Someone could also find it useful to consider a review of hardware costs required for this project in order to see which materials can be used in similar projects. A GitHub repo providing materials to control an RC car autonomously by using Raspberry Pi and Neural Networks.","title":"Miscellaneous"},{"location":"p06-robot-sets/","text":"Robot sets Below is the list of sets that one could consider to use either as a base to build a vehicle for the Future Engineers competition or as a model to learn and practise skills to participate in the competition. SunFounder PiCar-V Kit V2 . Pay attention to the fact that there are two DC motors driving the vehicle - this is not allowed by the competition rules. LEGO Mindstorms EV3 and Raspbery Pi with a camera. One of the possible ways to connect them is by BlueTooth but don't use this way for the competition. Fischertechnik Robotics Competition Set . It could be hard to build a steering mechanism with this set. But the ROBOTCS TXT Controller supports programming on Python . MATRIX Mini Starter Robot Kit and the Pixy2 camera . LEGO Education SPIKE Prime with OpenMV camera. Here is instructions to connect these devices together . After that it will require to deploy a special library to the OpenMV camera -- see examples here . MakeBlock Ultimate 2.0 Kit and Raspberry Pi with a camera. CViC - a set for participation in the autonomous cars competitions for school students. LEGO EV3 Computer Vision Add-on - an extension kit allowing to connect RaspberryPI and EV3 by UART.","title":"Robot sets"},{"location":"p06-robot-sets/#robot-sets","text":"Below is the list of sets that one could consider to use either as a base to build a vehicle for the Future Engineers competition or as a model to learn and practise skills to participate in the competition. SunFounder PiCar-V Kit V2 . Pay attention to the fact that there are two DC motors driving the vehicle - this is not allowed by the competition rules. LEGO Mindstorms EV3 and Raspbery Pi with a camera. One of the possible ways to connect them is by BlueTooth but don't use this way for the competition. Fischertechnik Robotics Competition Set . It could be hard to build a steering mechanism with this set. But the ROBOTCS TXT Controller supports programming on Python . MATRIX Mini Starter Robot Kit and the Pixy2 camera . LEGO Education SPIKE Prime with OpenMV camera. Here is instructions to connect these devices together . After that it will require to deploy a special library to the OpenMV camera -- see examples here . MakeBlock Ultimate 2.0 Kit and Raspberry Pi with a camera. CViC - a set for participation in the autonomous cars competitions for school students. LEGO EV3 Computer Vision Add-on - an extension kit allowing to connect RaspberryPI and EV3 by UART.","title":"Robot sets"}]}